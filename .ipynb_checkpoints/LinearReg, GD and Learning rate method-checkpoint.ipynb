{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By pf\n",
    "\n",
    "# Numerical modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Ipython modules\n",
    "import IPython\n",
    "import ipywidgets\n",
    "from ipywidgets import interact, interactive, interact_manual\n",
    "\n",
    "#custom packages\n",
    "#from packages.data_generation import polydata_generation\n",
    "from packages.data_generation import DG\n",
    "from packages.Regressors import LinearReg\n",
    "from packages.Util import GD, CrossVal\n",
    "from packages.Regularizers import L1, L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "point_num= 100 # number of points\n",
    "noise= 0.5 \n",
    "w_true= np.array([1, 3]) #real weights when producing labels\n",
    "X, Y= DG(point_num, noise, w_true.size).polydata_generation(w_true) # this is depicted in packages\n",
    "fig, ax= plt.subplots(1,1, figsize=(12,6))\n",
    "#Y = np.atleast_2d(Y).T     #plot also works if let Y be 2d.\n",
    "ax.plot(X[:,-1], Y, '*')\n",
    "ax.set_title('Generated Data')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "The update process of the (steepest)gradient descent is described as: \n",
    "$$w^{t+1}=w^t-\\eta_k \\nabla L(w^t)$$\n",
    "where $L(w)$ is the total loss function at the position $w$. In this case it is equivalent to the **empirical risk**:\n",
    "$$L(w)=\\frac{1}{n}\\sum_{i=1}^{n}l(x_i,y_i,w)=\\frac{1}{n}\\|Y-Xw\\|_2^2$$\n",
    "$$l(x_i,y_i,w)=\\frac{1}{2}(w^Tx_i-y_i)^2$$\n",
    "In the case of linear regression, we easily obtain that\n",
    "$$w^{t+1}=w^t-\\frac{1}{n}\\eta_k X^T(Xw-y)=w^t-\\eta_t X^T(Xw-y)~~~where~\\eta_t=\\frac{1}{n}\\eta_k$$\n",
    "In the vanilla gradient descent, learning rate is set constant that $\\eta_k=\\eta_0$, however a constant learning rate is usually not a remedy for many practical problems. The computational complexity is $O(n_{\\text{iter}} \\cdot nd)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d125bc1b32054063b71d86fdab15ebc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='Learning rate:', max=1.0, min=0.1), IntSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.hyper_para_interact(eta, iter_n)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_widget = ipywidgets.FloatSlider(value=1e-1, min=1e-1, max=1., step=1e-1, description='Learning rate:')\n",
    "n_widget = ipywidgets.IntSlider(value=10, min=10, max=30., step=2, description='Iterations:')\n",
    "\n",
    "def hyper_para_interact(eta, iter_n):\n",
    "    opts = {'iter': iter_n,\n",
    "           'learning rate': \"None\",\n",
    "           \"eta0\": eta}\n",
    "    regressor = LinearReg(X, Y) # write in the self-created packages\n",
    "    closed_form_w = regressor.w_closedform()\n",
    "    w0 = np.zeros(X.shape[1])\n",
    "    trajectory = GD(w0, regressor, opts=opts) # in the Util.py under /packages directory\n",
    "    w_hat = trajectory[-1, :]\n",
    "    loss = regressor.test_loss(X, Y, w_hat)\n",
    "    \n",
    "    print('Closed form solution: ', closed_form_w)\n",
    "    print(f\"Solution by gradient descent: {w_hat}\")\n",
    "    print(f\"Training error: {loss}\")\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    ax.plot(X[:,-1], Y, '*', label=\"Real data\")\n",
    "    ax.plot(X[:,-1], np.dot(X, w_hat), label=\"Fitted solution\")\n",
    "    ax.plot(X[:,-1], np.dot(X, closed_form_w), label=\"Real solution\")\n",
    "    \"\"\"above we directly use training data to plot, we can do this because its linear over x, so we can always get a line,\n",
    "    if not linear, we might encounter weird case that plot looks very chaotic because the their values are not ordered.\n",
    "    \"\"\"\n",
    "    ax.legend(loc=\"upper left\", fontsize=13)\n",
    "    ax.set_title('Fitting of linear regression')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    \n",
    "interact(hyper_para_interact, eta=eta_widget, iter_n=n_widget) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "When updating weights during the gradient descent algorithm, instead of computing full gradient, we surrogate the method by evaluating the gradient on one data at each epoch. Each data provides an unbiased but oscillating estimate:\n",
    "$$\\mathbb{E}\\left[\\frac{\\partial L(x,y)}{\\partial \\hat{w}}\\right] \\approx \\frac{\\partial L(x_i, y_i)}{\\partial \\hat{w}} = x_i (x_i^\\top \\hat{w} - y_i).$$\n",
    "As the gradient depends on one sample so it is quite noisy and needs more iterations to converge. The computational complexity is $O(n_{\\text{iter}} \\cdot d)$.\n",
    "\n",
    "**Note**: Since the gradient at a single point is noisy, the learning rate is recommended to be smaller as approaching to the convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61917812498e47a58b0715405987b16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='Learning rate:', max=1.0, min=0.01, step=0.05), IntS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.hyper_para_interact(eta, iter_n)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_widget = ipywidgets.FloatSlider(value=1e-1, min=1e-2, max=1., step=5e-2, description='Learning rate:')\n",
    "n_widget = ipywidgets.IntSlider(value=10, min=10, max=50., step=1, description='Iterations:')\n",
    "\n",
    "def hyper_para_interact(eta, iter_n):\n",
    "    opts = {'iter': iter_n,\n",
    "           'learning rate': \"None\",\n",
    "           \"eta0\": eta,\n",
    "           \"batchsize\": 1}\n",
    "    regressor = LinearReg(X, Y)\n",
    "    closed_form_w = regressor.w_closedform()\n",
    "    w0 = np.zeros(X.shape[1])\n",
    "    trajectory = GD(w0, regressor, opts=opts) #integrated in the GD function with batchsize=1\n",
    "    w_hat = trajectory[-1, :]\n",
    "    print('Closed form solution: ', closed_form_w)\n",
    "    print(f\"Solution by gradient descent: {w_hat}\")\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    ax.plot(X[:,-1], Y, '*', label=\"Real data\")\n",
    "    ax.plot(X[:,-1], np.dot(X, w_hat), label=\"Fitted solution\")\n",
    "    ax.plot(X[:,-1], np.dot(X, closed_form_w), label=\"Real solution\")\n",
    "    ax.legend(loc=\"upper left\", fontsize=13)\n",
    "    ax.set_title('Fitting of linear regression')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    \n",
    "interact(hyper_para_interact, eta=eta_widget, iter_n=n_widget) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch Gradient Descent:\n",
    "A trade-off between full gradient descent (higher cost, lower variance), and stochastic gradient descent (lower cost, higher variance), is done by computing the gradient of a mini-batch of points i.e.\n",
    "$$ \\mathbb{E}\\left[\\frac{\\partial L(x,y)}{\\partial \\hat{w}}\\right] \\approx \\frac{1}{|B|} \\sum_{i\\in B} \\frac{\\partial L(x_i, y_i)}{\\partial \\hat{w}} = \\frac{1}{|B|} \\sum_{i\\in B} x_i (x_i^\\top \\hat{w} - y_i). $$ \n",
    "\n",
    "The computational complexity is $O(n_{\\text{iter}} \\cdot |B| d)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4393ac290594413bffacc87cb1dc052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='Learning rate:', max=1.0, min=0.1), IntSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.hyper_para_interact(eta, iter_n, batchsize)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_widget = ipywidgets.FloatSlider(value=1e-1, min=1e-1, max=1., step=1e-1, description='Learning rate:')\n",
    "n_widget = ipywidgets.IntSlider(value=10, min=10, max=50., step=1, description='Iterations:')\n",
    "bs_widget = ipywidgets.IntSlider(value=1, min=1, max=X.shape[0], step=1, description='Batch size')\n",
    "\n",
    "def hyper_para_interact(eta, iter_n, batchsize):\n",
    "    opts = {'iter': iter_n,\n",
    "           'learning rate': \"None\",\n",
    "           'batchsize': batchsize,\n",
    "           \"eta0\": eta}\n",
    "    regressor = LinearReg(X, Y)\n",
    "    closed_form_w = regressor.w_closedform()\n",
    "    w0 = np.zeros(X.shape[1])\n",
    "    trajectory = GD(w0, regressor, opts=opts) \n",
    "    w_hat = trajectory[-1, :]\n",
    "    loss = regressor.test_loss(X, Y, w_hat)\n",
    "    \n",
    "    print('Closed form solution: ', closed_form_w)\n",
    "    print(f\"Solution by gradient descent: {w_hat}\")\n",
    "    print(f\"Training loss: {loss}\")\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    ax.plot(X[:,-1], Y, '*', label=\"Real data\")\n",
    "    ax.plot(X[:,-1], np.dot(X, w_hat), label=\"Fitted solution\")\n",
    "    ax.plot(X[:,-1], np.dot(X, closed_form_w), label=\"Real solution\")\n",
    "    ax.legend(loc=\"upper left\", fontsize=13)\n",
    "    ax.set_title('Fitting of linear regression')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    \n",
    "interact(hyper_para_interact, eta=eta_widget, iter_n=n_widget, batchsize=bs_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some adaptive learning rates\n",
    "\n",
    "* AdaGrad:\n",
    "\n",
    "    $\\eta_k = \\frac{\\eta_0}{\\sqrt{\\sum_{j=0}^k g_j^2}}$, where $g_j$ is the 2-norm gradient of $L$ at time $j$. \n",
    "\n",
    "\n",
    "* Annealing (From Stochastic Approximation) \n",
    "\n",
    "    $\\eta_{k} = \\frac{\\eta_0}{(k+1)^\\alpha}$, with $\\alpha \\in (0.5, 1]$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792858db3b604764a2f818b7c7456553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='Initial learning rate', max=5.0, min=0.1), IntSlider…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interact_para(eta, n, batchsize, lrmode)>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_widget = ipywidgets.FloatSlider(value=1e-1, min=1e-1, max=5., step=1e-1, description='Initial learning rate')\n",
    "n_widget = ipywidgets.IntSlider(value=10, min=5, max=50., step=1, description='Iterations')\n",
    "bs_widget = ipywidgets.IntSlider(value=1, min=1, max=X.shape[0], step=1, description='Batch size')\n",
    "lr_mode = ipywidgets.Dropdown(options=['Adagrad', 'Annealing', 'None'], value='None', description='Learning rate method')\n",
    "\n",
    "def interact_para(eta, n, batchsize, lrmode):\n",
    "    opts = {'iter': n,\n",
    "           'eta0': eta,\n",
    "            'batchsize': batchsize,\n",
    "           'learning rate': lrmode} #add one more key to the dictionary for learning rate choice.\n",
    "    \n",
    "    w0 = np.zeros(X.shape[1])\n",
    "    regressor = LinearReg(X, Y)\n",
    "    closed_form_w = regressor.w_closedform()\n",
    "    trajectory = GD(w0, regressor, opts=opts)\n",
    "    w_hat = trajectory[-1, :]\n",
    "    \n",
    "    print('Closed form solution: ', closed_form_w)\n",
    "    print(f\"Solution by gradient descent: {w_hat}\")\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    ax.plot(X[:,-1], Y, '*', label=\"Real data\")\n",
    "    ax.plot(X[:,-1], np.dot(X, w_hat), label=\"Fitted solution\")\n",
    "    ax.plot(X[:,-1], np.dot(X, closed_form_w), label=\"Real solution\")\n",
    "    ax.legend(loc=\"upper left\", fontsize=13)\n",
    "    ax.set_title('Fitting of linear regression')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "interact(interact_para, eta=eta_widget, n=n_widget, batchsize=bs_widget, lrmode=lr_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
