{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Environment and Collect Initial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5, Reward: 9.0\n",
      "Episode 10, Reward: 30.0\n",
      "Episode 15, Reward: 31.0\n",
      "Episode 20, Reward: 32.0\n",
      "Episode 25, Reward: 33.0\n",
      "Episode 30, Reward: 39.0\n",
      "Episode 35, Reward: 22.0\n",
      "Episode 40, Reward: 25.0\n",
      "Episode 45, Reward: 80.0\n",
      "Episode 50, Reward: 85.0\n",
      "Episode 55, Reward: 59.0\n",
      "Episode 60, Reward: 46.0\n",
      "Episode 65, Reward: 91.0\n",
      "Episode 70, Reward: 345.0\n",
      "Episode 75, Reward: 281.0\n",
      "Episode 80, Reward: 258.0\n",
      "Episode 85, Reward: 261.0\n",
      "Episode 90, Reward: 354.0\n",
      "Episode 95, Reward: 117.0\n",
      "Episode 100, Reward: 179.0\n",
      "Episode 105, Reward: 59.0\n",
      "Episode 110, Reward: 721.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]  # 4 for CartPole\n",
    "action_dim = env.action_space.n  # 2 for CartPole\n",
    "\n",
    "# Define a simple policy network for initial data collection\n",
    "class InitialPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, action_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        logits = self.forward(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "# Function to collect trajectories\n",
    "def collect_trajectories(policy, env, num_trajectories=10, max_steps=500):\n",
    "    trajectories = []\n",
    "    \n",
    "    for _ in range(num_trajectories):\n",
    "        states, actions, rewards, dones, s_next, log_probs = [], [], [], [], [], []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        while not done and step < max_steps:\n",
    "            action, log_prob = policy.act(state)\n",
    "            dones.append(done)\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            s_next.append(next_state)\n",
    "            log_probs.append(log_prob)\n",
    "            \n",
    "            state = next_state\n",
    "            step += 1\n",
    "        \n",
    "        # Store the trajectory\n",
    "        trajectories.append({\n",
    "            'states': np.array(states),\n",
    "            'actions': np.array(actions),\n",
    "            'rewards': np.array(rewards),\n",
    "            \"dones\": np.array(dones),\n",
    "            \"next_states\": np.array(s_next),\n",
    "            \"log_probs\": np.array(log_probs),\n",
    "            'length': len(states),\n",
    "            'return': sum(rewards)\n",
    "        })\n",
    "    \n",
    "    return trajectories\n",
    "\n",
    "# Initialize and train a basic policy to collect diverse trajectories\n",
    "initial_policy = InitialPolicy(state_dim, action_dim)\n",
    "optimizer = optim.Adam(initial_policy.parameters(), lr=0.01)\n",
    "gamma = 0.99\n",
    "# Train the initial policy for a few episodes to get somewhat reasonable behavior\n",
    "for episode in range(500):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    actions = []\n",
    "    states = []\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        logits = initial_policy(state_tensor)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        next_state, reward, done, truncated , _= env.step(action.item())\n",
    "        episode_reward += reward\n",
    "        states.append(state_tensor)\n",
    "        actions.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "\n",
    "    # Normalize returns for stability\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "    # Update policy once per episode\n",
    "    policy_loss = []\n",
    "    for log_prob, R in zip(actions, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "        \n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (episode + 1) % 5 == 0:\n",
    "        print(f\"Episode {episode+1}, Reward: {episode_reward}\")\n",
    "    if episode_reward >= 1000: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 100 trajectories\n"
     ]
    }
   ],
   "source": [
    "trajectories = collect_trajectories(initial_policy, env, num_trajectories=100)\n",
    "print(f\"Collected {len(trajectories)} trajectories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Human Preference Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 200 preference pairs\n"
     ]
    }
   ],
   "source": [
    "# Simulate human preferences based on various criteria\n",
    "def generate_human_preferences(trajectories, num_pairs=200):\n",
    "    \"\"\"\n",
    "    Generate synthetic human preferences based on multiple criteria:\n",
    "    1. Higher return (total reward)\n",
    "    2. Less extreme cart positions (closer to center)\n",
    "    3. Less extreme pole angles\n",
    "    4. Smoother actions (less switching)\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    n = len(trajectories)\n",
    "    \n",
    "    for _ in range(num_pairs):\n",
    "        # Randomly select two different trajectories\n",
    "        i, j = random.sample(range(n), 2)\n",
    "        traj1, traj2 = trajectories[i], trajectories[j]\n",
    "        \n",
    "        # Calculate preference metrics\n",
    "        # 1. Return (higher is better)\n",
    "        return1, return2 = traj1['return'], traj2['return']\n",
    "        \n",
    "        # 2. Cart position stability (lower absolute position is better)\n",
    "        pos_stability1 = np.mean(np.abs([s[0] for s in traj1['states']]))\n",
    "        pos_stability2 = np.mean(np.abs([s[0] for s in traj2['states']]))\n",
    "        \n",
    "        # 3. Pole angle stability (lower absolute angle is better)\n",
    "        angle_stability1 = np.mean(np.abs([s[2] for s in traj1['states']]))\n",
    "        angle_stability2 = np.mean(np.abs([s[2] for s in traj2['states']]))\n",
    "        \n",
    "        # 4. Action smoothness (fewer changes is better)\n",
    "        action_changes1 = np.sum(np.abs(np.diff(traj1['actions'])))\n",
    "        action_changes2 = np.sum(np.abs(np.diff(traj2['actions'])))\n",
    "        \n",
    "        # Normalize by trajectory length for fair comparison\n",
    "        action_changes1 /= max(1, len(traj1['actions']) - 1)\n",
    "        action_changes2 /= max(1, len(traj2['actions']) - 1)\n",
    "        \n",
    "        # Combine metrics with weights\n",
    "        score1 = (0.5 * return1 - \n",
    "                  0.2 * pos_stability1 - \n",
    "                  0.2 * angle_stability1 - \n",
    "                  0.1 * action_changes1)\n",
    "        \n",
    "        score2 = (0.5 * return2 - \n",
    "                  0.2 * pos_stability2 - \n",
    "                  0.2 * angle_stability2 - \n",
    "                  0.1 * action_changes2)\n",
    "        \n",
    "        # Determine preference (1 means traj1 is preferred, 0 means traj2 is preferred)\n",
    "        preference = 1 if score1 > score2 else 0\n",
    "        \n",
    "        # Store the pair with preference\n",
    "        pairs.append((traj1, traj2, preference))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Generate preference pairs\n",
    "preference_pairs = generate_human_preferences(trajectories)\n",
    "print(f\"Generated {len(preference_pairs)} preference pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 77.3828\n",
      "Epoch 20, Loss: 44.2642\n",
      "Epoch 30, Loss: 34.1429\n",
      "Epoch 40, Loss: 30.7154\n",
      "Epoch 50, Loss: 29.3170\n",
      "Epoch 60, Loss: 27.6882\n",
      "Epoch 70, Loss: 26.8126\n",
      "Epoch 80, Loss: 24.9261\n",
      "Epoch 90, Loss: 23.4280\n",
      "Epoch 100, Loss: 21.8602\n",
      "Reward model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Define the reward model\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.state_encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        \n",
    "        self.action_encoder = nn.Embedding(action_dim, 8)\n",
    "        \n",
    "        self.combined = nn.Sequential(\n",
    "            nn.Linear(32 + 8, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, states, actions):\n",
    "        # states: [batch_size, state_dim]\n",
    "        # actions: [batch_size]\n",
    "        state_features = self.state_encoder(states)\n",
    "        action_features = self.action_encoder(actions)\n",
    "        combined = torch.cat([state_features, action_features], dim=1)\n",
    "        return self.combined(combined).squeeze(-1)\n",
    "    \n",
    "    def get_reward(self, state, action):\n",
    "        # For single state-action pair\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action = torch.LongTensor([action])\n",
    "        return self.forward(state, action).item()\n",
    "\n",
    "# Function to compute trajectory reward\n",
    "def compute_trajectory_reward(reward_model, trajectory):\n",
    "    states = torch.FloatTensor(trajectory['states'])\n",
    "    actions = torch.LongTensor(trajectory['actions'])\n",
    "    return reward_model(states, actions).sum().item()\n",
    "\n",
    "# Train the reward model on preference pairs\n",
    "def train_reward_model(reward_model, preference_pairs, num_epochs=100, batch_size=32):\n",
    "    optimizer = optim.Adam(reward_model.parameters(), lr=0.001)\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        random.shuffle(preference_pairs)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for i in range(0, len(preference_pairs), batch_size):\n",
    "            batch = preference_pairs[i:i+batch_size]\n",
    "            batch_loss = 0.0\n",
    "            \n",
    "            for traj1, traj2, preference in batch:\n",
    "                # Convert trajectories to tensors\n",
    "                states1 = torch.FloatTensor(traj1['states'])\n",
    "                actions1 = torch.LongTensor(traj1['actions'])\n",
    "                states2 = torch.FloatTensor(traj2['states'])\n",
    "                actions2 = torch.LongTensor(traj2['actions'])\n",
    "                \n",
    "                # Get rewards for each trajectory\n",
    "                rewards1 = reward_model(states1, actions1).mean()\n",
    "                rewards2 = reward_model(states2, actions2).mean()\n",
    "                \n",
    "                # Compute the Bradley-Terry preference loss\n",
    "                logits = rewards1 - rewards2\n",
    "                \n",
    "                # Use binary cross-entropy loss with logits\n",
    "                loss = bce_loss(logits, torch.tensor(float(preference)) )\n",
    "                \n",
    "                batch_loss += loss\n",
    "            \n",
    "            # Update the reward model\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += batch_loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    return reward_model\n",
    "\n",
    "# Initialize and train the reward model\n",
    "reward_model = RewardModel(state_dim, action_dim)\n",
    "reward_model = train_reward_model(reward_model, preference_pairs)\n",
    "print(\"Reward model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement PPO and Fine-tune Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc_policy = nn.Linear(hidden_dim, action_dim)\n",
    "        self.fc_value = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.torch.nn.ReLU(self.fc1(x))\n",
    "        policy_logits = self.fc_policy(x)\n",
    "        policy = torch.softmax(policy_logits, dim=-1)\n",
    "        value = self.fc_value(x)\n",
    "        return policy, value\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=3e-4, gamma=0.99, clip_epsilon=0.2, update_epochs=4):\n",
    "        self.ac_net = ActorCriticNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.ac_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.update_epochs = update_epochs\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        policy, value = self.ac_net(state)\n",
    "        m = Categorical(policy)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action), value\n",
    "\n",
    "    def compute_returns_and_advantages(self, trajectories):\n",
    "        all_returns = []\n",
    "        all_advantages = []\n",
    "        for traj in trajectories:\n",
    "            returns = []\n",
    "            G = 0\n",
    "            for (_, _, _, r, _, _, _) in reversed(traj):\n",
    "                G = r + self.gamma * G\n",
    "                returns.insert(0, G)\n",
    "            returns = torch.tensor(returns, dtype=torch.float).to(device)\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "            advantages = []\n",
    "            for (i, (_, _, _, _, value, _, _)) in enumerate(traj):\n",
    "                advantages.append(returns[i] - value.item())\n",
    "            all_returns.append(returns)\n",
    "            all_advantages.append(torch.tensor(advantages, dtype=torch.float).to(device))\n",
    "        return all_returns, all_advantages\n",
    "\n",
    "    def update_td(self, trajectories):\n",
    "        # Extract all data from transitions into tensors\n",
    "        states = trajectories[\"states\"]\n",
    "        actions = trajectories[\"actions\"]\n",
    "        rewards = trajectories[\"rewards\"]\n",
    "        dones = trajectories[\"dones\"]\n",
    "        next_states = trajectories[\"next_states\"]\n",
    "\n",
    "        old_log_probs = torch.stack([t[2] for t in trajectories]).to(device)\n",
    "        rewards = torch.FloatTensor([t[3] for t in trajectories]).to(device)\n",
    "        values = torch.stack([t[4] for t in trajectories]).to(device).squeeze()\n",
    "        \n",
    "        # Calculate TD targets and advantages\n",
    "        with torch.no_grad():\n",
    "            _, next_values = self.ac_net(next_states)\n",
    "            next_values = next_values.squeeze()\n",
    "            next_values = next_values * (1 - dones)\n",
    "            td_targets = rewards + self.gamma * next_values\n",
    "            advantages = td_targets - values.detach()\n",
    "            # Optional: Normalize advantages\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Perform multiple epochs of updates (typical for PPO)\n",
    "        for _ in range(self.update_epochs):\n",
    "            # Get current policy and values\n",
    "            policy, current_values = self.ac_net(states)\n",
    "            m = Categorical(policy)\n",
    "            new_log_probs = m.log_prob(actions)\n",
    "            \n",
    "            # Calculate ratios and PPO clipped objective\n",
    "            ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Value loss using TD targets\n",
    "            value_loss = nn.functional.mse_loss(current_values.squeeze(), td_targets)\n",
    "            \n",
    "            # Optional: Add entropy bonus for exploration\n",
    "            entropy_loss = -m.entropy().mean() * 0.01\n",
    "            \n",
    "            # Total loss\n",
    "            loss = policy_loss + 0.5 * value_loss + entropy_loss\n",
    "            \n",
    "            # Update network\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards_history)\n",
    "plt.title('PPO Training with Learned Reward Model')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the final policy\n",
    "def evaluate_policy(env, policy, num_episodes=10):\n",
    "    total_rewards = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action, _, _, _ = policy.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    return np.mean(total_rewards)\n",
    "\n",
    "avg_reward = evaluate_policy(env, ppo_policy)\n",
    "print(f\"Average reward over 10 episodes: {avg_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize and Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize a policy in action\n",
    "def visualize_policy(env, policy, reward_model=None, num_steps=500):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    model_rewards = 0\n",
    "    \n",
    "    while not done and step < num_steps:\n",
    "        env.render()\n",
    "        \n",
    "        # Get action from policy\n",
    "        if isinstance(policy, PPOPolicy):\n",
    "            action, _, _, _ = policy.get_action(state)\n",
    "        else:\n",
    "            action = policy.act(state)\n",
    "        \n",
    "        # Take action\n",
    "        next_state, env_reward, done, _ = env.step(action)\n",
    "        total_reward += env_reward\n",
    "        \n",
    "        # Get reward from model if available\n",
    "        if reward_model:\n",
    "            model_reward = reward_model.get_reward(state, action)\n",
    "            model_rewards += model_reward\n",
    "            print(f\"Step {step}, Env Reward: {env_reward}, Model Reward: {model_reward:.4f}\")\n",
    "        else:\n",
    "            print(f\"Step {step}, Env Reward: {env_reward}\")\n",
    "        \n",
    "        state = next_state\n",
    "        step += 1\n",
    "    \n",
    "    env.close()\n",
    "    print(f\"Total environment reward: {total_reward}\")\n",
    "    if reward_model:\n",
    "        print(f\"Total model reward: {model_rewards:.4f}\")\n",
    "\n",
    "# Compare initial policy vs RLHF-trained policy\n",
    "print(\"Visualizing initial policy:\")\n",
    "visualize_policy(env, initial_policy)\n",
    "\n",
    "print(\"\\nVisualizing RLHF-trained policy:\")\n",
    "visualize_policy(env, ppo_policy, reward_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set random seeds for reproducibility.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Suppose we have a toy vocabulary of size 10, and we model sequences of length 5.\n",
    "vocab_size = 10\n",
    "embed_dim = 8\n",
    "hidden_dim = 16\n",
    "seq_len = 5\n",
    "batch_size = 2\n",
    "\n",
    "# Define a simple language model using an LSTM.\n",
    "class ToyLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(ToyLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x has shape: (batch_size, seq_len)\n",
    "        emb = self.embedding(x)  # shape: (batch_size, seq_len, embed_dim)\n",
    "        lstm_out, _ = self.lstm(emb)  # shape: (batch_size, seq_len, hidden_dim)\n",
    "        logits = self.fc(lstm_out)  # shape: (batch_size, seq_len, vocab_size)\n",
    "        return logits\n",
    "\n",
    "# Initialize the model, loss function, and optimizer.\n",
    "model = ToyLanguageModel(vocab_size, embed_dim, hidden_dim)\n",
    "criterion = nn.CrossEntropyLoss()  # expects inputs of shape (N, vocab_size) and targets of shape (N,)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Generate a toy dataset.\n",
    "# In SFT, x is the prompt and y is the target output sequence.\n",
    "# For simplicity, we randomly generate x and y (in practice, these come from human demonstrations).\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))  # shape: (batch_size, seq_len)\n",
    "y = torch.randint(0, vocab_size, (batch_size, seq_len))  # target sequence\n",
    "\n",
    "# Forward pass: obtain logits for each token position.\n",
    "logits = model(x)  # shape: (batch_size, seq_len, vocab_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deepstate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
