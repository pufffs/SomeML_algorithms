{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, loss=6.811\n",
      "Epoch 20, loss=5.465\n",
      "Epoch 30, loss=4.360\n",
      "Epoch 40, loss=3.480\n",
      "Epoch 50, loss=2.829\n",
      "Reward model training complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_pairs = 10\n",
    "dim_trajectory = 2\n",
    "\n",
    "# Randomly generate some fake \"trajectories\"...\n",
    "trajectories = [np.random.randn(dim_trajectory).astype(np.float32)\n",
    "                for _ in range(2 * num_pairs)]\n",
    "pairs = []\n",
    "for i in range(num_pairs):\n",
    "    tau1 = trajectories[2*i]\n",
    "    tau2 = trajectories[2*i + 1]\n",
    "    # For demonstration, let's define a ground-truth \"secret\" weight\n",
    "    # that humans used to prefer one or the other:\n",
    "    # e.g. prefer bigger L2 norm\n",
    "    secret_weight = np.array([1.0, 1.0], dtype=np.float32)\n",
    "    val1 = np.dot(tau1, secret_weight)\n",
    "    val2 = np.dot(tau2, secret_weight)\n",
    "    label = 1 if val1 > val2 else 0  # τ1 preferred if val1 > val2\n",
    "    pairs.append((tau1, tau2, label))\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, input_dim]\n",
    "        return self.net(x).squeeze(-1)  # output shape [batch_size]\n",
    "\n",
    "reward_model = RewardModel(input_dim=dim_trajectory)\n",
    "optimizer_rm = optim.Adam(reward_model.parameters(), lr=1e-3)\n",
    "\n",
    "def pairwise_loss(tau1, tau2, label):\n",
    "    \"\"\"\n",
    "    tau1, tau2: [batch_size, input_dim]\n",
    "    label: [batch_size], 1 means τ1>τ2, 0 means τ2>τ1\n",
    "    Bradley–Terry logistic: -log σ(R(τ1) - R(τ2)) for label=1\n",
    "                            -log σ(R(τ2) - R(τ1)) for label=0\n",
    "    \"\"\"\n",
    "    r1 = reward_model(tau1)  # shape [batch_size]\n",
    "    r2 = reward_model(tau2)\n",
    "    logits = r1 - r2  # shape [batch_size]\n",
    "    # label=1 => want σ(logits) near 1\n",
    "    # label=0 => want σ(logits) near 0 => σ(-logits) near 1\n",
    "    # cross entropy:\n",
    "    #   if label=1 => -log σ(logits)\n",
    "    #   if label=0 => -log [1 - σ(logits)] = -log σ(-logits)\n",
    "    # Implementation trick: use BCEWithLogitsLoss\n",
    "    labels_tensor = label.float()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    loss = criterion(logits, labels_tensor)\n",
    "    return loss\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(pairs)\n",
    "    epoch_loss = 0.0\n",
    "    for (tau1_np, tau2_np, lbl) in pairs:\n",
    "        tau1_tensor = torch.from_numpy(tau1_np).unsqueeze(0)  # [1, dim]\n",
    "        tau2_tensor = torch.from_numpy(tau2_np).unsqueeze(0)  # [1, dim]\n",
    "        lbl_tensor  = torch.tensor([lbl], dtype=torch.float32) # shape [1]\n",
    "\n",
    "        optimizer_rm.zero_grad()\n",
    "        loss = pairwise_loss(tau1_tensor, tau2_tensor, lbl_tensor)\n",
    "        loss.backward()\n",
    "        optimizer_rm.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    if (epoch+1)%10==0:\n",
    "        print(f\"Epoch {epoch+1}, loss={epoch_loss:.3f}\")\n",
    "\n",
    "print(\"Reward model training complete.\\n\")\n",
    "\n",
    "class TinyEnv:\n",
    "    \"\"\"\n",
    "    Toy environment with a 2D state s. We let the agent take action a in {0,1,2,...}\n",
    "    We'll define the next state randomly. We'll run for a fixed # of steps.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.max_steps = 5\n",
    "        self.t = 0\n",
    "        self.state = np.zeros(2, dtype=np.float32)\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.state = np.random.randn(2).astype(np.float32)\n",
    "        return self.state\n",
    "    def step(self, action):\n",
    "        # For demonstration, random next state\n",
    "        next_state = np.random.randn(2).astype(np.float32)\n",
    "        self.state = next_state\n",
    "        self.t += 1\n",
    "        done = (self.t >= self.max_steps)\n",
    "        # We'll define the \"trajectory\" as the next state for reward\n",
    "        # In a real example, the trajectory would be the entire sequence, but\n",
    "        # we show how to just apply Rψ on the next state as a stand-in\n",
    "        r_tensor = torch.from_numpy(next_state).unsqueeze(0)\n",
    "        reward = reward_model(r_tensor).item()  # use the reward model\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# We'll define a very minimal policy network\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, action_dim=3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # logits\n",
    "\n",
    "policy = PolicyNet(input_dim=2, hidden_dim=16, action_dim=3)\n",
    "optimizer_pi = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "\n",
    "# For PPO, we need some approximation. We'll skip advantage estimation here\n",
    "# and do a naive policy gradient approach to keep it short.\n",
    "def compute_logprob(logits, action):\n",
    "    # logits shape [batch, act_dim], action shape [batch]\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    return torch.gather(log_probs, 1, action.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "def run_episode(env, policy):\n",
    "    states, actions, rewards, logprobs = [], [], [], []\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        s_t = torch.from_numpy(s).unsqueeze(0).float()\n",
    "        logits = policy(s_t)  # shape [1, action_dim]\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        a = dist.sample()  # random sample\n",
    "        lp = dist.log_prob(a)\n",
    "\n",
    "        ns, r, done, _ = env.step(a.item())\n",
    "        states.append(s)\n",
    "        actions.append(a.item())\n",
    "        rewards.append(r)\n",
    "        logprobs.append(lp)\n",
    "        s = ns\n",
    "    return states, actions, rewards, logprobs\n",
    "\n",
    "def update_policy(rewards, logprobs, gamma=0.99):\n",
    "    # simple REINFORCE\n",
    "    G = 0.0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma*G\n",
    "        returns.insert(0, G)\n",
    "    returns_t = torch.tensor(returns, dtype=torch.float32)\n",
    "    # scale for stable training\n",
    "    returns_t = (returns_t - returns_t.mean()) / (returns_t.std()+1e-8)\n",
    "\n",
    "    loss = 0.0\n",
    "    for lp, Gt in zip(logprobs, returns_t):\n",
    "        loss += -lp * Gt\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, total reward of rollout: -3.355\n",
      "Iteration 20, total reward of rollout: -3.533\n",
      "Iteration 30, total reward of rollout: 2.175\n",
      "Iteration 40, total reward of rollout: 2.396\n",
      "Iteration 50, total reward of rollout: 3.520\n",
      "Done with toy PPO training using the learned reward model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = TinyEnv()\n",
    "\n",
    "for iter_i in range(50):\n",
    "    # gather one \"rollout\" from environment\n",
    "    states, actions, rewards, logprobs = run_episode(env, policy)\n",
    "    loss_pg = update_policy(rewards, logprobs)\n",
    "    optimizer_pi.zero_grad()\n",
    "    loss_pg.backward()\n",
    "    optimizer_pi.step()\n",
    "    if (iter_i+1) % 10 == 0:\n",
    "        print(f\"Iteration {iter_i+1}, total reward of rollout: {sum(rewards):.3f}\")\n",
    "\n",
    "print(\"Done with toy PPO training using the learned reward model.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deepstate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
