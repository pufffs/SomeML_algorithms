{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "import itertools\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underpinning of Leaps and Bounds(LB) algorithm is that for any sets of independent variables $A,B$ and $A \\subseteq B$, we always have\n",
    "$$RSS(B) \\leq RSS(A)$$\n",
    "When we construct an inverse tree as described in (Furnival and\n",
    "Wilson, 1974), we do not need to compute the RSS for all nodes. We can skip some or all descendants of the node depending on RSS value of this node compared to previous nodes. But one obvious problem was also depicted in the paper that, when we choose to skip a k-varible node, there might be its 1-varible descendant who has the lowest RSS values among all subsets that contain 1 variable. In (Furnival and Wilson, 1974), authors construct two trees: one for bounds and one for regression. (Ni and Huo, 2006) constructs a pair tree which combines two trees and represents a pair of subsets at each node. This tree achieves the original leaps and bounds method in a single structure. I think this structure is more comprehensive and coding-friendly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can find from the above pair tree that, all descendants come from the second subsets of each node, and the number of descendants depend on the location of this node among its siblings. Inspired by the scheme to go through and tree, I develop a recursive algorithm that scans the tree by a top-down, left-to-right order by once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, I will only use the above 5-variable example to explain the code, it works for more variables for sure. When given a root node (12345), we can produce 4 descendants and label their locations:\n",
    "$$\\left \\{ \\begin{array}{l}\n",
    "(1234,1235,1) \\\\\n",
    "(123,1245,2) \\\\\n",
    "(12,1345,3) \\\\\n",
    "(1,2345,4)\n",
    "\\end{array}\\right.$$\n",
    "Then we loop over each node and calculate the RSS of both left and right subsets. e.g for the third node (12,1345,3), the $RSS(left)=RSS(12)$, $RSS(right)=RSS(1345)$. Since its location is $3$ which means it has two descendants:\n",
    "\n",
    "$1$. (134,135): compare to produce the probable best 3-variable subset.\n",
    "\n",
    "$2$. (13,145): compare to produce the probable best 2-variable subset or(and) 3-variable subset.\n",
    "\n",
    "So at the node (12,1345), we need to compare $RSS^2_{\\min}=RSS(12)$, $RSS^3_{\\min}=\\min\\{RSS(123),RSS(124),RSS(125)\\}$ with $RSS(1345)$, Where $RSS^k_{\\min}$ is the minimum RSS value of k variabe model up to this node.\n",
    "$$RSS^k_{\\min} = inf\\{RSS(A):|A|=k\\}~~~up~to~the~current~node$$\n",
    "In the paper a directl left subset, say $RSS(12)$ is used to compare with the right subset, but in order to implement the iteration algorithm, we should update and use the minimum RSS value that contains corresponding number of variables. An intuitve example is that for the node (13,145), if we have $RSS(12)<RSS(145)<RSS(13)$ and we just compares $RSS(145)<RSS(13)$ then we should take a split here. But actually $RSS^2_{\\min}=RSS(12)<RSS(145)$ already indicates that it is not necessary to do it.\n",
    "\n",
    "Now go back to the node (12,1345), we have three cases:\n",
    "\n",
    "$1$: $RSS^2_{\\min}<RSS(1345)$, then all descendants should be skipped because we cannot find 2 or 3 variable subsets with smaller RSS values .\n",
    "\n",
    "$2$: $RSS^3_{\\min}<RSS(1345)<RSS^2_{\\min}$, then no need to calculate all 3-variable subsets over the descendants, but there might be a 2-variables subset with lower RSS value.\n",
    "\n",
    "$3$: $RSS(1345)<RSS^3_{\\min}<RSS^2_{\\min}$, then we need to calculate all the descendants.\n",
    "\n",
    "We can treat the left subset as bound subset to compare and decide whether to leap or not, and the right as regression subset since all descendants only come from the right subset, as described in the original paper by (Furnival and Wilson). Since all the nodes have the same descendant rule, all descendants come from only the right subset, we can use a function to iterate over those child nodes we donot want to leap, and then these child nodes will do the same process to leap or not for their child nodes.\n",
    "\n",
    "Everytime we always update the $RSS^k_{\\min}$ with the bound(left) subset because we can only reach at this stage if we choose not to leap. For example, at the node (3,45), we will need to compare $RSS(3)$ with $RSS^1_{\\min}$ because $RSS(3)$ might have the lower RSS value.\n",
    "\n",
    "For the regression(right) subset, we only update it with $RSS^k_{\\min}$ if all of its descendants are not skipped. For example, at (12,1345), if $RSS(1345)<RSS^3_{\\min}<RSS^2_{\\min}$, then there might have that $RSS(1345)<RSS^4_{\\min}$. Otherwise, we do not need to update with the right subset.\n",
    "\n",
    "In the algorithm I use two instance level hash maps to record $RSS^k_{\\min}$ and the indexes of variables that lead to the corresponding minimal RSS. What the algorithm does is to update the two hash maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nodes(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The class to represent node, with three attributes.\n",
    "        \"\"\"\n",
    "        self.LS = None # the index of left subset\n",
    "        self.RS = None # the index of left subset\n",
    "        self.loc = None # the location of the node, starting from 1, left to right.\n",
    "    \n",
    "class LB_select(object):\n",
    "    def __init__(self, X, Y, max_feature=None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.max_feature = max_feature #max number of features to print out.\n",
    "        self.num_col = X.shape[1]\n",
    "        \n",
    "        self.RSS_hashmap = {i:float('+Inf') for i in range(1,(self.num_col))} # initialise\n",
    "        self.coef_hashmap = {i:0 for i in range(1,(self.num_col))}\n",
    "        self.RSS_hashmap[self.num_col] = self.RSS() # RSS of full model is given\n",
    "        self.coef_hashmap[self.num_col] = np.arange(self.num_col)\n",
    "        \n",
    "    def RSS(self, ind=None): #calculate the RSS of the subset with certain index of features.\n",
    "        if ind is None:\n",
    "            ind = np.arange(self.num_col)\n",
    "        X = self.X[:,ind]\n",
    "        XTX_inv = np.linalg.pinv(np.dot(X.T, X))\n",
    "        H = np.dot(np.dot(X, XTX_inv), X.T)\n",
    "        mid = np.eye(H.shape[0]) - H\n",
    "        RSS = np.dot(np.dot(self.Y, mid), self.Y)\n",
    "        return RSS\n",
    "    \n",
    "    def muldelete(self, array, m):\n",
    "        \"\"\"\n",
    "        remove the last feature from the subset multiple times\n",
    "        (12345,3): (12345)->(1234)->(123)->(12)\n",
    "        this is used to obtain the bound(left) subset.\n",
    "        \"\"\"\n",
    "        for i in range(m):\n",
    "            array = np.delete(array, -1)\n",
    "        return array\n",
    "    \n",
    "    def allocate(self, array, t, cut=0):\n",
    "        L = []\n",
    "        for i in range(1+cut,t): # cut here is used to decide how many descendants we would like to skip.\n",
    "            node = Nodes() # define node class for each node\n",
    "            node.LS = self.muldelete(array, i) # obtain the left subset index\n",
    "            node.RS = np.delete(array, -(i+1)) # obtain the right subset\n",
    "            node.loc = i #obtain the location\n",
    "            L.append(node)\n",
    "        return L # return a list of nodes.\n",
    "    \n",
    "    def fit(self, array=None, t=None, cut=0):\n",
    "        if array is None:\n",
    "            array = np.arange(self.num_col) # for the root node split.\n",
    "        if t is None:\n",
    "            t = self.num_col\n",
    "        \n",
    "        for node in self.allocate(array, t, cut): # this gives the children nodes\n",
    "            k_LS = len(node.LS)\n",
    "            k_RS = len(node.RS)\n",
    "            RSS_left = self.RSS(node.LS) # RSS of left subset\n",
    "            RSS_right = self.RSS(node.RS) # RSS of right subset\n",
    "            \n",
    "            if RSS_left < self.RSS_hashmap[k_LS]: # always update with the left subset\n",
    "                self.RSS_hashmap[k_LS] = RSS_left\n",
    "                self.coef_hashmap[k_LS] = node.LS\n",
    "            \n",
    "            cut_ = 0\n",
    "            \"\"\"\n",
    "            compare RSS of right subset with those RSS of smaller number of features as described in the above write-up.\n",
    "            Then decide how many descendants to cut.\n",
    "            \"\"\"\n",
    "            for i in range(k_LS, k_RS):\n",
    "                if self.RSS_hashmap[i] < RSS_right:\n",
    "                    cut = k_RS - i\n",
    "                    cut_ = cut if cut > cut_ else cut_\n",
    "                    \n",
    "            if not cut_ and RSS_right < self.RSS_hashmap[k_RS]: #only update with the right subset if no descendant is cut.\n",
    "                self.RSS_hashmap[k_RS] = min(RSS_right, self.RSS_hashmap[k_RS])\n",
    "                self.coef_hashmap[k_RS] = node.RS\n",
    "                \n",
    "            rest = node.loc - 1 - cut_  # the number of rest descendants of this node.\n",
    "            \n",
    "            if rest: # recur only if there exsits at least one descendant after cutting.\n",
    "                self.fit(node.RS, node.loc, cut_)\n",
    "                \n",
    "    def best_subsets(self): # just for printing\n",
    "        if self.max_feature is None:\n",
    "            self.max_feature = self.num_col\n",
    "        for i in range(1, self.max_feature+1):\n",
    "            tup = (self.coef_hashmap[i], self.RSS_hashmap[i])\n",
    "            print(f\"The best subset containing {i} variables:{tup[0]}, RSS:{tup[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, coef = make_regression(n_samples=200,n_features=20,bias=0., coef=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best subset containing 1 variables:[0], RSS:5017666.10156715\n",
      "The best subset containing 2 variables:[0 1], RSS:5017242.608075809\n",
      "The best subset containing 3 variables:[0 1 2], RSS:3499764.0852203607\n",
      "The best subset containing 4 variables:[0 1 2 3], RSS:3482080.455687042\n",
      "The best subset containing 5 variables:[0 1 2 3 4], RSS:3466979.5167830884\n",
      "The best subset containing 6 variables:[0 1 2 3 4 5], RSS:2792359.239986906\n",
      "The best subset containing 7 variables:[0 1 2 3 4 5 6], RSS:2702336.7728171805\n",
      "The best subset containing 8 variables:[0 1 2 3 4 5 6 7], RSS:1880801.866600051\n",
      "The best subset containing 9 variables:[0 1 2 3 4 5 6 7 8], RSS:1880318.2051972367\n",
      "The best subset containing 10 variables:[0 1 2 3 4 5 6 7 8 9], RSS:1856716.6269774034\n",
      "The best subset containing 11 variables:[ 0  1  2  3  4  5  6  7  8  9 10], RSS:1265911.702946904\n",
      "The best subset containing 12 variables:[ 0  1  2  3  4  5  6  7  8  9 10 11], RSS:1265909.509988517\n",
      "The best subset containing 13 variables:[ 0  1  2  3  4  5  6  7  8  9 10 11 12], RSS:1215170.2658222555\n",
      "The best subset containing 14 variables:[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], RSS:1213903.058148696\n",
      "The best subset containing 15 variables:[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14], RSS:1206483.8934704603\n",
      "The best subset containing 16 variables:[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15], RSS:1200853.2489588796\n",
      "The best subset containing 17 variables:[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16], RSS:1128331.8824238873\n",
      "The best subset containing 18 variables:[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17], RSS:144077.29321019267\n",
      "The best subset containing 19 variables:[ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19], RSS:-3.1284700562984887e-09\n",
      "The best subset containing 20 variables:[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19], RSS:-1.8378927491066567e-09\n",
      "Time:  0.037961799999720824\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "obj = LB_select(X,Y)\n",
    "obj.fit()\n",
    "obj.best_subsets()\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "time1 = stop - start\n",
    "print('Time: ', time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method tries all $2^k-1$ subsets and choose the best models, I will use this as the benchmark to check my above algorithm and compare the computation effort the two methods need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 variable subset:[19], RSS:1905138.8327636449\n",
      "2 variable subset:[ 2 19], RSS:1317393.037414426\n",
      "3 variable subset:[ 0  2 19], RSS:1027758.6377753841\n",
      "4 variable subset:[ 0  2 17 19], RSS:820332.8242543285\n",
      "5 variable subset:[ 0  2 15 17 19], RSS:626511.9969780827\n",
      "6 variable subset:[ 0  2  9 15 17 19], RSS:396171.4194004401\n",
      "7 variable subset:[ 0  2  9 11 15 17 19], RSS:246213.73752790497\n",
      "8 variable subset:[ 0  2  9 10 11 15 17 19], RSS:84792.18767073881\n",
      "9 variable subset:[ 0  2  4  9 10 11 15 17 19], RSS:32653.29590576977\n",
      "10 variable subset:[ 0  2  4  6  9 10 11 15 17 19], RSS:-6.371819485362343e-10\n",
      "11 variable subset:[ 0  2  3  4  6  9 10 11 15 17 19], RSS:-1.5907865401280491e-09\n",
      "12 variable subset:[ 0  1  2  4  6  9 10 11 13 15 17 19], RSS:-2.7337598279793286e-09\n",
      "13 variable subset:[ 0  2  3  4  6  8  9 10 11 13 15 17 19], RSS:-1.1238109343908229e-08\n",
      "14 variable subset:[ 0  1  2  4  5  6  9 10 11 12 15 17 18 19], RSS:-4.73912671448239e-09\n",
      "15 variable subset:[ 0  2  4  6  7  8  9 10 11 12 14 15 17 18 19], RSS:-5.740693373241218e-09\n",
      "16 variable subset:[ 0  1  2  4  5  6  7  8  9 10 11 12 14 15 17 19], RSS:-5.135010293991513e-09\n",
      "17 variable subset:[ 0  2  3  4  5  6  7  9 10 11 12 13 14 15 17 18 19], RSS:-6.892721388079458e-09\n",
      "18 variable subset:[ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19], RSS:-2.8952314250690305e-09\n",
      "19 variable subset:[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19], RSS:-2.0591125508006024e-09\n",
      "20 variable subset:[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19], RSS:2.1170986640618507e-10\n",
      "Time:  574.5169554\n"
     ]
    }
   ],
   "source": [
    "def RSS(X,Y, ind):\n",
    "    X = X[:,ind]\n",
    "    XTX_inv = np.linalg.pinv(np.dot(X.T, X))\n",
    "    H = np.dot(np.dot(X, XTX_inv), X.T)\n",
    "    mid = np.eye(H.shape[0]) - H\n",
    "    RSS = np.dot(np.dot(Y, mid), Y)\n",
    "    coef = np.dot(np.dot(XTX_inv, X.T), Y)\n",
    "    return RSS\n",
    "\n",
    "start = timeit.default_timer()\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "num = X.shape[1]\n",
    "hm = {i:0 for i in range(1,num+1)}\n",
    "hm1 = {i:0 for i in range(1,num+1)}\n",
    "a = np.arange(num)\n",
    "for i in range(1, num+1):\n",
    "    rss0 = float(\"Inf\")\n",
    "    for combo in itertools.combinations(a, i):\n",
    "        combo = np.array(combo)\n",
    "        rss = RSS(X, Y, combo)\n",
    "        if rss < rss0:\n",
    "            rss0 = rss\n",
    "            hm[i] = combo\n",
    "            hm1[i] = rss0\n",
    "for i in range(1,num+1):\n",
    "    print(f\"{i} variable subset:{hm[i]}, RSS:{hm1[i]}\")\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "time2 = stop - start\n",
    "print('Time: ', stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be easily checked by eyes that above two methods give the same results for models containing different number of variables, but the computation time requires distinguishes hugely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|k|LB algorithm(s)|Try all possible models(s)|\n",
    "|-|-|-|\n",
    "|5|0.02|0.02|\n",
    "|10|0.1|0.54|\n",
    "|15|0.28|17.7|\n",
    "|20|12|forever|\n",
    "|25|30|forever|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "Furnival, George M., and Robert W. Wilson. “Regressions by Leaps and Bounds.” Technometrics 16, no. 4 (1974): 499–511. https://doi.org/10.2307/1267601.\n",
    "Ni, Xuelei & Huo, Xiaoming. (2006). Regression by enhanced leaps-and-bounds via additional optimality tests (LBOT). https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.215.7016&rep=rep1&type=pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
