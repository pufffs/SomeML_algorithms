{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient Theorem Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is: \n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\sum_{t=0}^{\\infty} \\gamma^t\\,r(s_t, a_t)\\right]\n",
    "$$\n",
    "\n",
    "We can write:\n",
    "$$\n",
    "J(\\theta) = \\int p_\\theta(\\tau)\\left[\\sum_{t=0}^{\\infty} \\gamma^t\\,r(s_t,a_t)\\right]\\,d\\tau.\n",
    "$$\n",
    "\n",
    "Taking the gradient with respect to $\\theta$:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\int p_\\theta(\\tau)\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right]d\\tau.\n",
    "$$\n",
    "Assuming we can interchange the gradient and the integral:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int \\nabla_\\theta p_\\theta(\\tau)\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right]d\\tau.\n",
    "$$\n",
    "\n",
    "Applying the Log-Likelihood trick gives:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int p_\\theta(\\tau)\\,\\nabla_\\theta \\log p_\\theta(\\tau) \\left[\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right]d\\tau.\n",
    "$$\n",
    "In expectation notation:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\nabla_\\theta \\log p_\\theta(\\tau) \\,\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right].\n",
    "$$\n",
    "\n",
    "\n",
    "Since the trajectory probability factorizes as\n",
    "$$\n",
    "p_\\theta(\\tau) = p(s_0)\\prod_{t=0}^{\\infty} \\left[\\pi_\\theta(a_t \\mid s_t) \\,P(s_{t+1} \\mid s_t,a_t)\\right],\n",
    "$$\n",
    "its logarithm is\n",
    "$$\n",
    "\\log p_\\theta(\\tau) = \\log p(s_0) + \\sum_{t=0}^{\\infty}\\Bigl[\\log \\pi_\\theta(a_t \\mid s_t) + \\log P(s_{t+1} \\mid s_t,a_t)\\Bigr].\n",
    "$$\n",
    "Only the terms $\\log \\pi_\\theta(a_t \\mid s_t)$ depend on $\\theta$. Therefore,\n",
    "$$\n",
    "\\nabla_\\theta \\log p_\\theta(\\tau) = \\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t).\n",
    "$$\n",
    "Substitute back into our gradient expression:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\left(\\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\right)\\left(\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right)\\right].\n",
    "$$\n",
    "\n",
    "For each time step t, decompose the total return as:\n",
    "$$\n",
    "\\sum_{m=0}^{\\infty}\\gamma^m\\,r(s_m,a_m)\n",
    "=\\underbrace{\\sum_{m=0}^{t-1}\\gamma^m\\,r(s_m,a_m)}_{\\text{past (independent of \\(a_t\\))}}\n",
    "+\\underbrace{\\sum_{m=t}^{\\infty}\\gamma^m\\,r(s_m,a_m)}_{\\text{future (dependent on \\(a_t\\))}}.\n",
    "$$\n",
    "Since the past portion is independent of $(a_t)$, its contribution to $(\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t))$ vanishes in expectation. Hence, we have:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\left(\\sum_{m=t}^{\\infty}\\gamma^m\\,r(s_m,a_m)\\right)\\right].\n",
    "$$\n",
    "\n",
    "Since the Q function is:\n",
    "$$\n",
    "Q^\\pi(s_t,a_t) = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty}\\gamma^k\\,r(s_{t+k},a_{t+k}) \\,\\Bigm|\\, s_t,a_t\\right].\n",
    "$$\n",
    "Thus, the partial sum $(\\sum_{m=t}^{\\infty}\\gamma^m\\,r(s_m,a_m)$ is an unbiased sample of $(Q^\\pi(s_t,a_t))$. Therefore,\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\,Q^\\pi(s_t,a_t)\\right].\n",
    "$$\n",
    "\n",
    "\n",
    "Each trajectory $(\\tau)$ consists of a sequence of state–action pairs $((s_0,a_0), (s_1,a_1), \\ldots)$. The sum over time steps is equivalent to taking an expectation with respect to the **discounted occupancy measure** $(\\mu_\\pi(s,a))$, which represents the (normalized) distribution of state–action pairs visited by $(\\pi_\\theta)$. Thus, we can write:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s,a) \\sim \\mu_\\pi}\\left[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)\\,Q^\\pi(s,a)\\right].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Final Policy Gradient Theorem**\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s,a) \\sim \\mu_\\pi}\\left[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)\\,Q^\\pi(s,a)\\right].\n",
    "}\n",
    "$$\n",
    "A common variance-reduced version uses the advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$:\n",
    "$$\n",
    "\\boxed{\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s,a) \\sim \\mu_\\pi}\\left[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)\\,A^\\pi(s,a)\\right].\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------------------\n",
    "# Environment: CartPole-v1\n",
    "# ---------------------\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]  # 4 for CartPole\n",
    "action_dim = env.action_space.n \n",
    "\n",
    "def train_online(agent, env, episodes=500, method='REINFORCE'):\n",
    "    rewards_per_episode = []\n",
    "    \n",
    "    # Set update frequency based on method\n",
    "    if method == 'REINFORCE':\n",
    "        update_freq = 'episode'\n",
    "    elif method == 'A2C':\n",
    "        update_freq = 'n_steps'\n",
    "        n_steps = 5  # Update every 5 steps\n",
    "    elif method == 'PPO':\n",
    "        update_freq = 'n_episodes'\n",
    "        n_episodes = 5  # Update after 5 episodes\n",
    "    elif method in ['NPG', 'TRPO']:\n",
    "        update_freq = 'episode'\n",
    "    \n",
    "    # Initialize buffers for different update frequencies\n",
    "    buffer = []\n",
    "    steps_since_update = 0\n",
    "    episodes_since_update = 0\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_traj = []\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if method in ['REINFORCE', 'NPG', 'TRPO']:\n",
    "                action, log_prob = agent.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                transition = (state, action, log_prob, reward, None, next_state, done or truncated)\n",
    "            elif method in ['A2C', 'PPO']:\n",
    "                action, log_prob, value = agent.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                transition = (state, action, log_prob, reward, value, next_state, done or truncated)\n",
    "            \n",
    "            episode_traj.append(transition)\n",
    "            buffer.append(transition)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Update based on frequency\n",
    "            if update_freq == 'n_steps':\n",
    "                steps_since_update += 1\n",
    "                if steps_since_update >= n_steps or done:\n",
    "                    if method == 'A2C':\n",
    "                        agent.update_td(buffer)  # Use TD targets\n",
    "                    buffer = []\n",
    "                    steps_since_update = 0\n",
    "        \n",
    "        rewards_per_episode.append(total_reward)\n",
    "        \n",
    "        # Update after episode if that's the chosen frequency\n",
    "        if update_freq == 'episode':\n",
    "            if method == 'REINFORCE':\n",
    "                rewards = [t[3] for t in episode_traj]\n",
    "                log_probs = [t[2] for t in episode_traj]\n",
    "                agent.update(rewards, log_probs)\n",
    "            elif method in ['NPG', 'TRPO']:\n",
    "                # For NPG/TRPO, we might need more data\n",
    "                data = [episode_traj]\n",
    "                rewards = [t[3] for t in episode_traj]\n",
    "                log_probs = [t[2] for t in episode_traj]\n",
    "                if method == 'NPG':\n",
    "                    agent.update(rewards, log_probs, data)\n",
    "                else:  # TRPO\n",
    "                    old_log_probs = [[t[2] for t in episode_traj]]\n",
    "                    agent.update([episode_traj], old_log_probs)\n",
    "        \n",
    "        # Update after N episodes for methods like PPO\n",
    "        if update_freq == 'n_episodes':\n",
    "            episodes_since_update += 1\n",
    "            if episodes_since_update >= n_episodes:\n",
    "                if method == 'PPO':\n",
    "                    agent.update_td(buffer)  # Use TD targets\n",
    "                buffer = []\n",
    "                episodes_since_update = 0\n",
    "        \n",
    "        if (ep+1) % 50 == 0:\n",
    "            print(f\"Online Episode {ep+1}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    return rewards_per_episode\n",
    "\n",
    "def collect_trajectories(agent, env, num_episodes=50, method='REINFORCE'):\n",
    "    trajectories = []\n",
    "    for ep in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        traj = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            if method in ['REINFORCE', 'NPG', 'TRPO']:\n",
    "                action, log_prob = agent.select_action(state)\n",
    "                # For actor-critic based methods, we may get additional info.\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                traj.append((state, action, log_prob, reward, None, next_state, done or truncated))\n",
    "            elif method in ['A2C', 'PPO']:\n",
    "                action, log_prob, value = agent.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                traj.append((state, action, log_prob, reward, value, next_state, done or truncated))\n",
    "            state = next_state\n",
    "        trajectories.append(traj)\n",
    "    return trajectories\n",
    "\n",
    "def train_offline(agent, offline_data, epochs=10, method='REINFORCE'):\n",
    "    # offline_data is a list of trajectories pre-collected\n",
    "    rewards_list = []\n",
    "    for epoch in range(epochs):\n",
    "        total_reward = 0\n",
    "        for traj in offline_data:\n",
    "            # For offline REINFORCE, we compute the update over each trajectory in the dataset.\n",
    "            if method == 'REINFORCE':\n",
    "                rewards = [entry[3] for entry in traj]\n",
    "                log_probs = [entry[2] for entry in traj]\n",
    "                total_reward += sum(rewards)\n",
    "                agent.update(rewards, log_probs)\n",
    "            elif method == 'A2C':\n",
    "                total_reward += sum([entry[3] for entry in traj])\n",
    "                agent.update(traj)\n",
    "            elif method == 'NPG':\n",
    "                rewards = [entry[3] for entry in traj]\n",
    "                log_probs = [entry[2] for entry in traj]\n",
    "                data = offline_data  # use entire offline dataset\n",
    "                agent.update(rewards, log_probs, data)\n",
    "            elif method == 'TRPO':\n",
    "                old_log_probs = [ [entry[2] for entry in traj] ]\n",
    "                agent.update([traj], old_log_probs)\n",
    "                total_reward += sum([entry[3] for entry in traj])\n",
    "            elif method == 'PPO':\n",
    "                agent.update([traj])\n",
    "                total_reward += sum([entry[3] for entry in traj])\n",
    "        rewards_list.append(total_reward / len(offline_data))\n",
    "        print(f\"Offline Epoch {epoch+1}, Average Total Reward per Trajectory: {rewards_list[-1]:.2f}\")\n",
    "    return rewards_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Objective:**\n",
    "\n",
    "The expected return is given by\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T-1}\\gamma^t\\,r(s_t,a_t)\\right],\n",
    "$$\n",
    "\n",
    "where $ \\tau = (s_0,a_0,\\dots,s_{T-1},a_{T-1}) $ is a complete episode.\n",
    "\n",
    "**Policy Gradient Theorem (using Monte Carlo return):**\n",
    "\n",
    "The gradient is\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) \\, G_t\\right],\n",
    "$$\n",
    "\n",
    "with the return\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{k=t}^{T-1}\\gamma^{k-t}\\,r(s_k,a_k).\n",
    "$$\n",
    "\n",
    "**Update Rule:**\n",
    "\n",
    "For each episode, update\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T-1} G_t \\, \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Policy network for REINFORCE (outputs action probabilities)\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc_policy = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.ReLU(self.fc1(x))\n",
    "        logits = self.fc_policy(x)\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "\n",
    "class REINFORCEAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-2, gamma=0.99):\n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.policy_net(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "    \n",
    "    def update(self, rewards, log_probs):\n",
    "        # Compute returns G_t = r_t + gamma*r_{t+1} + ... for each time step t.\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns, dtype=torch.float).to(device) #[r_0,r_0+r_1*\\gamma,r_0+r_1*\\gamma+r_2*\\gamma^2,...]\n",
    "        # Optional normalization for stability:\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        loss = 0\n",
    "        for log_prob, G in zip(log_probs, returns):\n",
    "            loss -= log_prob * G  # Note: gradient ascent on expected return.\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = REINFORCEAgent(state_dim, action_dim) \n",
    "online_rewards = train_online(agent, env, episodes=500, method=\"REINFORCE\")\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(online_rewards, label=\"Online Rewards\")\n",
    "plt.xlabel(\"Episode / Epoch\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Critic (A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture:**\n",
    "\n",
    "We have an actor (policy) and a critic (value function). The critic estimates\n",
    "\n",
    "$$\n",
    "V^\\pi(s) \\approx \\mathbb{E}\\left[G_t \\mid s_t = s\\right].\n",
    "$$\n",
    "\n",
    "**Advantage Estimate:**\n",
    "\n",
    "A common choice is the one-step temporal difference (TD) error:\n",
    "\n",
    "$$\n",
    "A_t = r(s_t,a_t) + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t).\n",
    "$$\n",
    "\n",
    "**Loss Functions:**\n",
    "\n",
    "- **Actor Loss:**\n",
    "\n",
    "$$\n",
    "L_{\\text{actor}}(\\theta) = -\\mathbb{E}\\left[\\log \\pi_\\theta(a_t\\mid s_t)\\,A_t\\right].\n",
    "$$\n",
    "\n",
    "- **Critic Loss:**\n",
    "\n",
    "$$\n",
    "L_{\\text{critic}}(\\phi) = \\frac{1}{2}\\mathbb{E}\\left[\\left(r(s_t,a_t) + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t)\\right)^2\\right].\n",
    "$$\n",
    "\n",
    "**Update Rules:**\n",
    "\n",
    "- **Actor Update:**\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha\\,\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) \\, A_t.\n",
    "$$\n",
    "\n",
    "- **Critic Update:**\n",
    "\n",
    "$$\n",
    "\\phi \\leftarrow \\phi - \\beta\\,\\nabla_\\phi L_{\\text{critic}}(\\phi).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic network for A2C/PPO (shared encoder, two heads: policy & value)\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc_policy = nn.Linear(hidden_dim, action_dim)\n",
    "        self.fc_value = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.torch.nn.ReLU(self.fc1(x))\n",
    "        policy_logits = self.fc_policy(x)\n",
    "        policy = torch.softmax(policy_logits, dim=-1)\n",
    "        value = self.fc_value(x)\n",
    "        return policy, value\n",
    "\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_dim, action_dim, actor_lr=1e-3, critic_lr=1e-3, gamma=0.99):\n",
    "        self.ac_net = ActorCriticNetwork(state_dim, action_dim).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.ac_net.fc_policy.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.ac_net.fc_value.parameters(), lr=critic_lr)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        policy, value = self.ac_net(state)\n",
    "        m = Categorical(policy)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action), value\n",
    "\n",
    "    def update(self, trajectory):\n",
    "        # trajectory: list of (state, action, log_prob, reward, value, next_state, done)\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for (_, _, _, reward, _, _, _) in reversed(trajectory):\n",
    "            R = reward + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, dtype=torch.float).to(device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        for (s, a, log_prob, reward, value, s_next, done), R in zip(trajectory, returns):\n",
    "            advantage = R - value.item()\n",
    "            policy_losses.append(-log_prob * advantage)\n",
    "            value_losses.append(nn.functional.mse_loss(value, torch.tensor([R], dtype=torch.float).to(device)))\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss = torch.stack(policy_losses).sum()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss = torch.stack(value_losses).sum()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "    # Add this method to the A2CAgent class\n",
    "    def update_td(self, transitions):\n",
    "        # Extract all data from transitions into tensors\n",
    "        states = torch.FloatTensor([t[0] for t in transitions]).to(device)\n",
    "        actions = torch.LongTensor([t[1] for t in transitions]).to(device)\n",
    "        log_probs = torch.stack([t[2] for t in transitions]).to(device)\n",
    "        rewards = torch.FloatTensor([t[3] for t in transitions]).to(device)\n",
    "        values = torch.stack([t[4] for t in transitions]).to(device).squeeze()\n",
    "        next_states = torch.FloatTensor([t[5] for t in transitions]).to(device)\n",
    "        dones = torch.FloatTensor([float(t[6]) for t in transitions]).to(device)\n",
    "        \n",
    "        # Calculate next state values for TD targets\n",
    "        with torch.no_grad():\n",
    "            _, next_values = self.ac_net(next_states)\n",
    "            next_values = next_values.squeeze()\n",
    "            next_values = next_values * (1 - dones)\n",
    "        \n",
    "        # Calculate TD targets: r + gamma * V(s')\n",
    "        td_targets = rewards + self.gamma * next_values\n",
    "        \n",
    "        # Calculate advantages: TD error\n",
    "        advantages = td_targets - values.detach()\n",
    "        \n",
    "        # Optional: Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Policy loss using advantages\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        # Value loss using TD targets\n",
    "        value_loss = nn.functional.mse_loss(values, td_targets)\n",
    "        \n",
    "        # Update actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward(retain_graph=True)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.critic_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Policy Gradient (NPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Idea:**\n",
    "\n",
    "The standard gradient is preconditioned by the inverse Fisher information matrix $F(\\theta) $ to obtain the natural gradient:\n",
    "\n",
    "$$\n",
    "\\Delta \\theta = F(\\theta)^{-1} \\, \\nabla_\\theta J(\\theta),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "F(\\theta) = \\mathbb{E}_{s \\sim d^\\pi,\\, a \\sim \\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s) \\, \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)^\\top\\right].\n",
    "$$\n",
    "\n",
    "**Update Rule:**\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\, \\Delta \\theta = \\theta + \\alpha \\, F(\\theta)^{-1} \\, \\nabla_\\theta J(\\theta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-2, gamma=0.99):\n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)  # Base optimizer (we will precondition manually)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.policy_net(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "    \n",
    "    def compute_gradient(self, rewards, log_probs):\n",
    "        # Similar to REINFORCE, compute the gradient vector.\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns, dtype=torch.float).to(device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        grad_vector = []\n",
    "        for log_prob, G in zip(log_probs, returns):\n",
    "            grad_vector.append(G * log_prob)\n",
    "        # Sum gradients (each log_prob is a scalar multiplied by the gradient vector of the policy network)\n",
    "        # We compute the gradient of the loss with respect to parameters manually.\n",
    "        loss = -torch.stack(grad_vector).sum()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gather the gradient vector from all parameters into a single flattened vector.\n",
    "        grad_flat = torch.cat([p.grad.view(-1) for p in self.policy_net.parameters()])\n",
    "        return grad_flat.detach()\n",
    "    \n",
    "    def estimate_fisher(self, data, num_samples=10):\n",
    "        # Estimate the Fisher Information Matrix from collected data.\n",
    "        grads = []\n",
    "        for _ in range(num_samples):\n",
    "            # Sample one trajectory from data (offline dataset) or one rollout\n",
    "            traj = data[np.random.randint(len(data))]\n",
    "            # For simplicity, use the first time step.\n",
    "            s, a, _, _, _, _, _ = traj[0]\n",
    "            s = torch.FloatTensor(s).unsqueeze(0).to(device)\n",
    "            probs = self.policy_net(s)\n",
    "            m = Categorical(probs)\n",
    "            log_prob = m.log_prob(torch.tensor(a).to(device))\n",
    "            self.policy_net.zero_grad()\n",
    "            log_prob.backward()\n",
    "            grad_sample = torch.cat([p.grad.view(-1) for p in self.policy_net.parameters()])\n",
    "            grads.append(grad_sample.unsqueeze(0))\n",
    "        grads = torch.cat(grads, dim=0)  # [num_samples, D]\n",
    "        # Fisher is approximated by the outer product of the gradients\n",
    "        F = torch.matmul(grads.t(), grads) / num_samples\n",
    "        return F.detach()\n",
    "    \n",
    "    def update(self, rewards, log_probs, data):\n",
    "        # Compute regular REINFORCE gradient:\n",
    "        g = self.compute_gradient(rewards, log_probs)\n",
    "        # Estimate Fisher matrix using some offline data:\n",
    "        F = self.estimate_fisher(data)\n",
    "        # Solve for natural gradient: Δθ = F^{-1}g\n",
    "        # For simplicity, we use torch.linalg.solve (assuming F is invertible)\n",
    "        natural_grad = torch.linalg.solve(F, g)\n",
    "        # Update parameters manually:\n",
    "        index = 0\n",
    "        for p in self.policy_net.parameters():\n",
    "            numel = p.numel()\n",
    "            p.data += 1e-2 * natural_grad[index:index+numel].view_as(p)\n",
    "            index += numel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust Region Policy Optimization (TRPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of reinforcement learning is to maximize the expected return:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]=\\eta(\\pi_{\\theta})\n",
    "$$\n",
    "Consider the expected return under the new policy $\\pi_{\\theta'}$:\n",
    "\n",
    "$$\n",
    "J(\\theta') = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta'}}[R(\\tau)]\n",
    "$$\n",
    "\n",
    "Using importance sampling with respect to the old policy $\\pi_{\\theta_{\\text{old}}}$, we rewrite:\n",
    "\n",
    "$$\n",
    "J(\\theta') = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\frac{\\pi_{\\theta'}(\\tau)}{\\pi_{\\theta_{\\text{old}}}(\\tau)}R(\\tau)\\right]\n",
    "$$\n",
    "\n",
    "However, directly computing $\\frac{\\pi_{\\theta'}(\\tau)}{\\pi_{\\theta_{\\text{old}}}(\\tau)}$ over entire trajectories is challenging. Therefore, TRPO approximates this by using single-step importance sampling and the advantage function $A^{\\pi_{\\theta_{\\text{old}}}}(s,a)$:\n",
    "\n",
    "$$\n",
    "J(\\theta') \\approx \\eta(\\pi_{\\theta_{\\text{old}}}) + \\sum_s d^{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_a \\pi_{\\theta'}(a|s)A^{\\pi_{\\theta_{\\text{old}}}}(s,a)\n",
    "$$\n",
    "\n",
    "Here, we define the surrogate objective clearly as:\n",
    "\n",
    "$$\n",
    "L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) = \\eta(\\pi_{\\theta_{\\text{old}}}) + \\sum_s d^{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_a \\pi_{\\theta'}(a|s)A^{\\pi_{\\theta_{\\text{old}}}}(s,a)\n",
    "$$\n",
    "\n",
    "Recall the **Conservative Policy Iteration (CPI)** lower bound:\n",
    "$$\n",
    "L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) \\geq \\eta(\\pi_{\\theta_{\\text{old}}}) + \\sum_{s} d^{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_{a}\\pi_{\\theta'}(a|s)A^{\\pi_{\\theta_{\\text{old}}}}(s,a) - \\frac{2\\gamma\\epsilon}{(1-\\gamma)^2} D_{\\text{KL}}^{\\text{max}}(\\pi_{\\theta_{\\text{old}}}\\|\\pi_{\\theta'})\n",
    "$$\n",
    "The CPI bound above is theoretically insightful but practically restrictive due to the term $D_{\\text{KL}}^{\\text{max}}(\\pi||\\pi')$, which requires bounding the divergence at **every state**. \n",
    "\n",
    "In practice, we replace the \"max KL divergence\" with an **average KL divergence** to simplify computation:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}^{\\text{max}}(\\pi_{\\theta_{\\text{old}}}\\|\\pi_{\\theta'}) \\approx \\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)\\|\\pi_{\\theta'}(\\cdot|s))]\n",
    "$$\n",
    "\n",
    "This leads to a more manageable lower bound:\n",
    "\n",
    "$$\n",
    "L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) \\geq \\eta(\\pi_{\\theta_{\\text{old}}}) + \\sum_{s} d^{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_{a}\\pi_{\\theta'}(a|s)A^{\\pi_{\\theta_{\\text{old}}}}(s,a) - \\frac{2\\gamma\\epsilon}{(1-\\gamma)^2}\\cdot\\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)\\|\\pi_{\\theta'}(\\cdot|s))]\n",
    "$$\n",
    "To ensure policy improvement, we aim to maximize the lower bound above. Equivalently, we can pose this as a constrained optimization problem. Specifically, we seek a new policy $\\pi'$ that maximizes (since $\\eta(\\pi_{\\theta_{\\text{old}}})$ is constant):\n",
    "\n",
    "$$\n",
    "L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) = \\sum_{s} d^{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_{a}\\pi_{\\theta'}(a|s)A^{\\pi_{\\theta_{\\text{old}}}}(s,a)=\\mathbb{E}_{s,a \\sim \\pi_{\\theta_{\\text{old}}}} \\left[ \\frac{\\pi_{\\theta'}(a|s)}{\\pi_{\\theta_{old}}(a|s)}A^{\\pi_{\\theta_{\\text{old}}}}(s,a)\\right]\n",
    "$$\n",
    "\n",
    "subject to a constraint on the KL divergence between the old policy and the new policy:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)\\|\\pi_{\\theta'}(\\cdot|s))] \\leq \\delta\n",
    "$$\n",
    "\n",
    "Here, $\\delta$ is a hyperparameter chosen to control how aggressively the policy can change in each update.\n",
    "\n",
    "Thus, the optimization becomes explicitly:\n",
    "\n",
    "$$\n",
    "\\max_{\\pi_{\\theta'}} L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) \\quad\\text{s.t.}\\quad \\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)\\|\\pi_{\\theta'}(\\cdot|s))] \\leq \\delta\n",
    "$$\n",
    "\n",
    "We approximate the surrogate objective around $\\theta_{\\text{old}}$ with a first-order (linear) Taylor expansion:\n",
    "\n",
    "$$\n",
    "L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) \\approx L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta_{\\text{old}}}) + \\nabla_{\\theta} L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta})_{\\theta =\\theta_{old}}^\\top (\\theta' - \\theta_{\\text{old}})\n",
    "$$\n",
    "\n",
    "For the KL divergence constraint, we apply a second-order (quadratic) Taylor expansion around $\\theta_{\\text{old}}$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}}\\left[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)||\\pi_{\\theta'}(\\cdot|s))\\right] \\approx \\frac{1}{2}(\\theta' - \\theta_{\\text{old}})^\\top F(\\theta_{\\text{old}})(\\theta' - \\theta_{\\text{old}})\n",
    "$$\n",
    "\n",
    "where the Fisher Information Matrix (FIM), $F(\\theta_{\\text{old}})$, is defined as:\n",
    "\n",
    "$$\n",
    "F(\\theta_{\\text{old}}) = \\mathbb{E}_{s,a \\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^\\top\\right]\\Big|_{\\theta=\\theta_{\\text{old}}}\n",
    "$$\n",
    "\n",
    "Combining the above approximations yields a simpler constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\max_{\\theta'} \\nabla_{\\theta} L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta})_{\\theta =\\theta_{old}}^\\top(\\theta' - \\theta_{\\text{old}}) \\quad \\text{subject to} \\quad \\frac{1}{2}(\\theta' - \\theta_{\\text{old}})^\\top F(\\theta_{\\text{old}})(\\theta' - \\theta_{\\text{old}}) \\leq \\delta\n",
    "$$\n",
    "\n",
    "Using the method of Lagrange multipliers, we derive the optimal policy update step explicitly as:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\sqrt{\\frac{2\\delta}{g^\\top F^{-1} g}} F^{-1} g\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $g = \\nabla_{\\theta} L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta})_{\\theta =\\theta_{old}}$ is the policy gradient evaluated at $\\theta_{old}$ by using the logrithm trick.\n",
    "- $F$ is the Fisher Information Matrix evaluated at $\\theta_{\\text{old}}$.\n",
    "\n",
    "In practice, directly computing the inverse $F^{-1}$ is computationally expensive. Thus, TRPO uses the conjugate gradient method to approximate the product $F^{-1} g$ efficiently without explicitly computing the inverse.\n",
    "\n",
    "---\n",
    "**TRPO Algorithm**\n",
    "\n",
    "**Given:** initial policy parameters $\\theta_0$, KL-divergence constraint parameter $\\delta$\n",
    "\n",
    "**for** iteration $k=0,1,2,\\dots$ **do**:\n",
    "\n",
    "1. **Collect trajectories** by executing policy $\\pi_{\\theta_k}(a|s)$.\n",
    "\n",
    "2. **Compute advantages** $A^{\\pi_{\\theta_k}}(s,a)$ using collected data.\n",
    "\n",
    "3. **Compute policy gradient:**\n",
    "$$\n",
    "g = \\mathbb{E}_{s,a \\sim \\pi_{\\theta_k}}\\left[\\nabla_{\\theta'}\\log\\pi_{\\theta'}(a|s)\\big|_{\\theta'=\\theta_k} A^{\\pi_{\\theta_k}}(s,a)\\right]\n",
    "$$\n",
    "\n",
    "4. **Estimate Fisher Information Matrix (FIM)**:\n",
    "$$\n",
    "F(\\theta_k) = \\mathbb{E}_{s,a \\sim \\pi_{\\theta_k}}\\left[\\nabla_{\\theta_k}\\log\\pi_{\\theta_k}(a|s)\\nabla_{\\theta_k}\\log\\pi_{\\theta_k}(a|s)^\\top\\right]\n",
    "$$\n",
    "\n",
    "5. **Compute policy update direction** by approximately solving:\n",
    "$$\n",
    "F(\\theta_k)x = g\n",
    "$$\n",
    "using the **conjugate gradient method**.\n",
    "\n",
    "6. **Update policy parameters**:\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k + \\sqrt{\\frac{2\\delta}{g^\\top x}}\\,x\n",
    "$$\n",
    "\n",
    "**end for**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, max_kl=1e-2, cg_iters=10, damping=0.1):\n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        ).to(device)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=1e-3)\n",
    "        self.gamma = gamma\n",
    "        self.max_kl = max_kl\n",
    "        self.cg_iters = cg_iters\n",
    "        self.damping = damping\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.policy_net(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        value = self.value_net(state)\n",
    "        return action.item(), m.log_prob(action), value\n",
    "    \n",
    "    def compute_advantages(self, trajectories):\n",
    "        # Compute TD advantages for all trajectories\n",
    "        states = torch.cat([torch.FloatTensor([t[0] for t in traj]) for traj in trajectories]).to(device)\n",
    "        rewards = torch.cat([torch.FloatTensor([t[3] for t in traj]) for traj in trajectories]).to(device)\n",
    "        next_states = torch.cat([torch.FloatTensor([t[5] for t in traj]) for traj in trajectories]).to(device)\n",
    "        dones = torch.cat([torch.FloatTensor([float(t[6]) for t in traj]) for traj in trajectories]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            values = self.value_net(states).squeeze()\n",
    "            next_values = self.value_net(next_states).squeeze()\n",
    "            next_values = next_values * (1 - dones)\n",
    "            td_targets = rewards + self.gamma * next_values\n",
    "            advantages = (td_targets - values).detach()\n",
    "            # Normalize advantages\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "            \n",
    "        return states, advantages, td_targets\n",
    "    \n",
    "    def compute_policy_gradient(self, states, actions, advantages):\n",
    "        # Compute policy gradient: ∇_θ J(θ)\n",
    "        self.policy_net.zero_grad()\n",
    "        probs = self.policy_net(states)\n",
    "        dist = Categorical(probs)\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        policy_loss.backward()\n",
    "        \n",
    "        # Get flattened gradient\n",
    "        policy_grad = torch.cat([param.grad.view(-1) for param in self.policy_net.parameters()]).detach()\n",
    "        return policy_grad\n",
    "    \n",
    "    def fisher_vector_product(self, states, vector):\n",
    "        # Compute product of Fisher Information Matrix and vector: F·v\n",
    "        vector = torch.FloatTensor(vector).to(device)\n",
    "        \n",
    "        # Compute KL divergence\n",
    "        probs = self.policy_net(states)\n",
    "        dist = Categorical(probs)\n",
    "        \n",
    "        # Create a differentiable function for KL\n",
    "        kl = torch.mean(torch.distributions.kl_divergence(\n",
    "            Categorical(probs.detach()), dist))\n",
    "        \n",
    "        # Compute gradient of KL w.r.t. policy parameters\n",
    "        self.policy_net.zero_grad()\n",
    "        kl.backward(create_graph=True)\n",
    "        \n",
    "        # Compute Hessian-vector product using gradients\n",
    "        grads = torch.cat([param.grad.view(-1) for param in self.policy_net.parameters()])\n",
    "        flat_grad_kl = grads.detach()\n",
    "        \n",
    "        # Compute the product of gradients and vector\n",
    "        grad_kl_v = (flat_grad_kl * vector).sum()\n",
    "        \n",
    "        # Compute the Hessian-vector product\n",
    "        self.policy_net.zero_grad()\n",
    "        grad_kl_v.backward()\n",
    "        \n",
    "        # Get the Hessian-vector product\n",
    "        hessian_vector_product = torch.cat([param.grad.view(-1) for param in self.policy_net.parameters()]).detach()\n",
    "        \n",
    "        return hessian_vector_product + self.damping * vector\n",
    "    \n",
    "    def conjugate_gradient(self, states, b, max_iter=10):\n",
    "        # Solve Fx = b using conjugate gradient method\n",
    "        x = torch.zeros_like(b)\n",
    "        r = b.clone()\n",
    "        p = b.clone()\n",
    "        r_dot_r = torch.dot(r, r)\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            Ap = self.fisher_vector_product(states, p)\n",
    "            alpha = r_dot_r / torch.dot(p, Ap)\n",
    "            x += alpha * p\n",
    "            r -= alpha * Ap\n",
    "            \n",
    "            new_r_dot_r = torch.dot(r, r)\n",
    "            beta = new_r_dot_r / r_dot_r\n",
    "            p = r + beta * p\n",
    "            r_dot_r = new_r_dot_r\n",
    "            \n",
    "            if r_dot_r < 1e-10:\n",
    "                break\n",
    "                \n",
    "        return x\n",
    "    \n",
    "    def update_policy(self, states, actions, advantages):\n",
    "        # Compute policy gradient\n",
    "        policy_grad = self.compute_policy_gradient(states, actions, advantages)\n",
    "        \n",
    "        # Compute natural gradient direction using conjugate gradient\n",
    "        nat_grad = self.conjugate_gradient(states, policy_grad, max_iter=self.cg_iters)\n",
    "        \n",
    "        # Compute step size\n",
    "        step_dir = nat_grad\n",
    "        \n",
    "        # Compute the maximum step size that satisfies the KL constraint\n",
    "        shs = 0.5 * torch.dot(step_dir, self.fisher_vector_product(states, step_dir))\n",
    "        lm = torch.sqrt(self.max_kl / shs)\n",
    "        full_step = lm * step_dir\n",
    "        \n",
    "        # Perform line search to ensure improvement and KL constraint\n",
    "        self.line_search(states, actions, advantages, full_step)\n",
    "    \n",
    "    def line_search(self, states, actions, advantages, full_step, max_backtracks=10, accept_ratio=0.1):\n",
    "        # Get current policy loss\n",
    "        with torch.no_grad():\n",
    "            probs = self.policy_net(states)\n",
    "            dist = Categorical(probs)\n",
    "            log_probs = dist.log_prob(actions)\n",
    "            old_loss = -(log_probs * advantages).mean().item()\n",
    "        \n",
    "        # Get flattened parameters\n",
    "        prev_params = torch.cat([param.data.view(-1) for param in self.policy_net.parameters()])\n",
    "        \n",
    "        for i in range(max_backtracks):\n",
    "            # Compute new parameters\n",
    "            step_size = 0.5**i\n",
    "            new_params = prev_params + step_size * full_step\n",
    "            \n",
    "            # Update model parameters\n",
    "            idx = 0\n",
    "            for param in self.policy_net.parameters():\n",
    "                param_size = param.numel()\n",
    "                param.data.copy_(new_params[idx:idx+param_size].view(param.shape))\n",
    "                idx += param_size\n",
    "            \n",
    "            # Compute new loss and KL\n",
    "            with torch.no_grad():\n",
    "                new_probs = self.policy_net(states)\n",
    "                new_dist = Categorical(new_probs)\n",
    "                new_log_probs = new_dist.log_prob(actions)\n",
    "                new_loss = -(new_log_probs * advantages).mean().item()\n",
    "                \n",
    "                # Compute KL divergence\n",
    "                kl = torch.mean(torch.distributions.kl_divergence(\n",
    "                    Categorical(probs.detach()), new_dist)).item()\n",
    "            \n",
    "            # Check if update satisfies constraints\n",
    "            if kl < self.max_kl and new_loss <= old_loss:\n",
    "                return True\n",
    "        \n",
    "        # If no step satisfies constraints, revert to old parameters\n",
    "        idx = 0\n",
    "        for param in self.policy_net.parameters():\n",
    "            param_size = param.numel()\n",
    "            param.data.copy_(prev_params[idx:idx+param_size].view(param.shape))\n",
    "            idx += param_size\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def update(self, trajectories):\n",
    "        # Update value network\n",
    "        states, advantages, td_targets = self.compute_advantages(trajectories)\n",
    "        \n",
    "        # Update value function\n",
    "        for _ in range(5):  # Multiple epochs for value function\n",
    "            self.value_optimizer.zero_grad()\n",
    "            values = self.value_net(states).squeeze()\n",
    "            value_loss = nn.functional.mse_loss(values, td_targets)\n",
    "            value_loss.backward()\n",
    "            self.value_optimizer.step()\n",
    "        \n",
    "        # Extract actions\n",
    "        actions = torch.cat([torch.LongTensor([t[1] for t in traj]) for traj in trajectories]).to(device)\n",
    "        \n",
    "        # Update policy using TRPO\n",
    "        self.update_policy(states, actions, advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trust Region Policy Optimization (TRPO) solves the following constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\max_{\\theta'} L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) = \\mathbb{E}_{s,a\\sim\\pi_{\\theta_{\\text{old}}}}\\left[\\frac{\\pi_{\\theta'}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)}A^{\\pi_{\\theta_{\\text{old}}}}(s,a)\\right]\n",
    "$$\n",
    "\n",
    "subject to the KL-divergence constraint:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{s\\sim d^{\\pi_{\\theta_{\\text{old}}}}}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)||\\pi_{\\theta'}(\\cdot|s))]\\leq \\delta\n",
    "$$\n",
    "\n",
    "TRPO is computationally expensive due to the constraint on KL divergence. PPO simplifies TRPO by using an easier-to-compute penalty mechanism that achieves similar stability without explicitly enforcing KL constraints.\n",
    "\n",
    "Specifically, PPO introduces the following \"surrogate objective\" (without explicit constraints):\n",
    "\n",
    "$$\n",
    "L^{\\text{PPO}}(\\theta) = \\mathbb{E}_{s,a\\sim\\pi_{\\theta_{\\text{old}}}}\\left[\n",
    "r(\\theta)A^{\\pi_{\\theta_{\\text{old}}}}(s,a) - \\beta D_{\\text{KL}}\\bigl(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)||\\pi_{\\theta}(\\cdot|s)\\bigr)\n",
    "\\right]\n",
    "$$\n",
    "While the explicit KL penalty term in PPO is easy, tuning the coefficient $\\beta$ can be tricky. PPO further simplifies this by introducing a \"clipped\" surrogate objective:\n",
    "\n",
    "$$\n",
    "L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_{s,a\\sim\\pi_{\\theta_{\\text{old}}}}\\left[\n",
    "\\min\\left(\n",
    "r(\\theta)A^{\\pi_{\\theta_{\\text{old}}}}(s,a),\\,\\text{clip}(r(\\theta),1-\\epsilon,1+\\epsilon)A^{\\pi_{\\theta_{\\text{old}}}}(s,a)\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "This removes the need for explicitly adjusting a penalty coefficient $\\beta$, thus making PPO easier to implement and tune.\n",
    "\n",
    "---\n",
    "\n",
    "#### How PPO Clipping Acts as an Implicit Penalty\n",
    "\n",
    "Although PPO doesn’t explicitly add a penalty term like KL divergence, the clipping function implicitly penalizes overly large policy changes. Here's how:\n",
    "\n",
    "(1) When Advantage $A^{\\pi_{\\theta_k}}(s,a)>0$ (action better than average):\n",
    "\n",
    "- If $r(\\theta)$ is too large (> $1+\\epsilon$), the objective clips and doesn't reward further increases.\n",
    "- If $r(\\theta)$ is too small (<$1-\\epsilon$), it naturally reduces the objective since we're multiplying a smaller ratio with a positive advantage.\n",
    "\n",
    "(2) When Advantage $A^{\\pi_{\\theta_k}}(s,a)<0$ (action worse than average):\n",
    "\n",
    "- If $r(\\theta)$ is too small (<$1-\\epsilon$), it clips at $1-\\epsilon$, Now multiplying a negative advantage by a larger number ($1-\\epsilon$) creates a less-negative objective than the original ratio would, thus effectively limiting the \"benefit\" of aggressively reducing the probability of bad actions (but still allowing a controlled reduction).\n",
    "\n",
    "- If $r(\\theta)$ is becomes large (> $1+\\epsilon$) for negative advantages, PPO explicitly does not clip upwards. Here, the large ratio multiplied by negative advantage naturally yields a very negative objective value, strongly discouraging that undesirable policy update.\n",
    "\n",
    "\n",
    "Thus, clipping effectively serves as an implicit \"penalty\" mechanism, guiding the policy to update cautiously.\n",
    "\n",
    "---\n",
    "**PPO Algorithm**\n",
    "\n",
    "**Given:** Initial policy parameters $\\theta_0$, clipping parameter $\\epsilon$ (e.g., 0.2), learning rate $\\eta$\n",
    "\n",
    "**for** iteration $k = 0, 1, 2, \\dots$ **do**:\n",
    "\n",
    "1. Collect trajectories using policy $\\pi_{\\theta_k}(a|s)$.\n",
    "\n",
    "2. Compute advantage estimates $A^{\\pi_{\\theta_k}}(s,a)$.\n",
    "\n",
    "3. **for** each gradient update epoch **do**:\n",
    "\n",
    "   - Compute probability ratios:\n",
    "     $$\n",
    "     r(\\theta) = \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}\n",
    "     $$\n",
    "\n",
    "   - Maximize the clipped PPO objective:\n",
    "     $$\n",
    "     L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_{s,a\\sim \\pi_{\\theta_k}}\\left[\n",
    "     \\min\\left(r(\\theta)A^{\\pi_{\\theta_k}}(s,a),\\,\\text{clip}(r(\\theta),1-\\epsilon,1+\\epsilon)A^{\\pi_{\\theta_k}}(s,a)\\right)\n",
    "     \\right]\n",
    "     $$\n",
    "\n",
    "   by performing gradient ascent step:\n",
    "   $$\n",
    "   \\theta \\leftarrow \\theta + \\eta \\nabla_{\\theta} L^{\\text{CLIP}}(\\theta)\n",
    "   $$\n",
    "\n",
    "3. Update the old policy:\n",
    "   $$\n",
    "   \\theta_{k+1}\\leftarrow \\theta\n",
    "   $$\n",
    "\n",
    "**end for**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=3e-4, gamma=0.99, clip_epsilon=0.2, update_epochs=4):\n",
    "        self.ac_net = ActorCriticNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.ac_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.update_epochs = update_epochs\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        policy, value = self.ac_net(state)\n",
    "        m = Categorical(policy)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action), value\n",
    "\n",
    "    def compute_returns_and_advantages(self, trajectories):\n",
    "        all_returns = []\n",
    "        all_advantages = []\n",
    "        for traj in trajectories:\n",
    "            returns = []\n",
    "            G = 0\n",
    "            for (_, _, _, r, _, _, _) in reversed(traj):\n",
    "                G = r + self.gamma * G\n",
    "                returns.insert(0, G)\n",
    "            returns = torch.tensor(returns, dtype=torch.float).to(device)\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "            advantages = []\n",
    "            for (i, (_, _, _, _, value, _, _)) in enumerate(traj):\n",
    "                advantages.append(returns[i] - value.item())\n",
    "            all_returns.append(returns)\n",
    "            all_advantages.append(torch.tensor(advantages, dtype=torch.float).to(device))\n",
    "        return all_returns, all_advantages\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        states, actions, old_log_probs, returns, advantages = [], [], [], [], []\n",
    "        for traj in trajectories:\n",
    "            for (s, a, logp, r, v, s_next, done) in traj:\n",
    "                states.append(s)\n",
    "                actions.append(a)\n",
    "                old_log_probs.append(logp)\n",
    "            ret, adv = self.compute_returns_and_advantages([traj])\n",
    "            returns += ret.tolist()\n",
    "            advantages += adv.tolist()\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        old_log_probs = torch.stack(old_log_probs).to(device)\n",
    "        returns = torch.tensor(returns, dtype=torch.float).to(device)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float).to(device)\n",
    "        \n",
    "        for _ in range(self.update_epochs):\n",
    "            policy, values = self.ac_net(states)\n",
    "            m = Categorical(policy)\n",
    "            new_log_probs = m.log_prob(actions)\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = nn.functional.mse_loss(values.squeeze(), returns)\n",
    "            entropy_loss = -m.entropy().mean()\n",
    "            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "    # Add this method to the PPOAgent class\n",
    "    def update_td(self, transitions):\n",
    "        # Extract all data from transitions into tensors\n",
    "        states = torch.FloatTensor([t[0] for t in transitions]).to(device)\n",
    "        actions = torch.LongTensor([t[1] for t in transitions]).to(device)\n",
    "        old_log_probs = torch.stack([t[2] for t in transitions]).to(device)\n",
    "        rewards = torch.FloatTensor([t[3] for t in transitions]).to(device)\n",
    "        values = torch.stack([t[4] for t in transitions]).to(device).squeeze()\n",
    "        next_states = torch.FloatTensor([t[5] for t in transitions]).to(device)\n",
    "        dones = torch.FloatTensor([float(t[6]) for t in transitions]).to(device)\n",
    "        \n",
    "        # Calculate TD targets and advantages\n",
    "        with torch.no_grad():\n",
    "            _, next_values = self.ac_net(next_states)\n",
    "            next_values = next_values.squeeze()\n",
    "            next_values = next_values * (1 - dones)\n",
    "            td_targets = rewards + self.gamma * next_values\n",
    "            advantages = td_targets - values.detach()\n",
    "            # Optional: Normalize advantages\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Perform multiple epochs of updates (typical for PPO)\n",
    "        for _ in range(self.update_epochs):\n",
    "            # Get current policy and values\n",
    "            policy, current_values = self.ac_net(states)\n",
    "            m = Categorical(policy)\n",
    "            new_log_probs = m.log_prob(actions)\n",
    "            \n",
    "            # Calculate ratios and PPO clipped objective\n",
    "            ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Value loss using TD targets\n",
    "            value_loss = nn.functional.mse_loss(current_values.squeeze(), td_targets)\n",
    "            \n",
    "            # Optional: Add entropy bonus for exploration\n",
    "            entropy_loss = -m.entropy().mean() * 0.01\n",
    "            \n",
    "            # Total loss\n",
    "            loss = policy_loss + 0.5 * value_loss + entropy_loss\n",
    "            \n",
    "            # Update network\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{Algorithm: Offline PPO} \\\\[1mm]\n",
    "\\textbf{Input:} \\; D = \\{(s_i, a_i, \\log\\pi_{\\text{old}}(a_i \\mid s_i),\\, A_i)\\}_{i=1}^N, \\quad \\text{Epochs } E, \\quad \\text{Mini-batch size } B, \\quad \\epsilon, \\quad \\alpha. \\\\[1mm]\n",
    "\\textbf{Initialize } \\theta \\quad \\text{(policy parameters)}. \\\\[1mm]\n",
    "\\textbf{For } \\text{epoch} = 1 \\textbf{ to } E \\textbf{ do:} \\\\\n",
    "\\quad \\text{Shuffle the dataset } D. \\\\[1mm]\n",
    "\\quad \\textbf{For each mini-batch } M \\subset D \\text{ of size } B \\textbf{ do:} \\\\[1mm]\n",
    "\\qquad \\textbf{For each sample } (s, a, \\log\\pi_{\\text{old}}, A) \\in M \\textbf{ do:} \\\\[1mm]\n",
    "\\qquad\\quad \\text{Compute } \\log\\pi_{\\theta}(a \\mid s). \\\\[1mm]\n",
    "\\qquad\\quad \\text{Compute the importance ratio: } \\quad r = \\exp\\Bigl(\\log\\pi_{\\theta}(a \\mid s) - \\log\\pi_{\\text{old}}(a \\mid s)\\Bigr). \\\\[1mm]\n",
    "\\qquad \\textbf{End For} \\\\[1mm]\n",
    "\\qquad \\text{Compute the surrogate losses:} \\\\[1mm]\n",
    "\\qquad \\quad L_1 = r \\cdot A, \\\\[1mm]\n",
    "\\qquad \\quad L_2 = \\operatorname{clip}\\bigl(r,\\, 1-\\epsilon,\\, 1+\\epsilon\\bigr) \\cdot A, \\\\[1mm]\n",
    "\\qquad \\quad L_{\\text{clip}} = \\frac{1}{|M|} \\sum_{(s,a,\\log\\pi_{\\text{old}},A)\\in M} \\min\\bigl(L_1, L_2\\bigr). \\\\[1mm]\n",
    "\\qquad \\text{Update } \\theta \\text{ via gradient descent:} \\\\[1mm]\n",
    "\\qquad \\quad \\theta \\leftarrow \\theta - \\alpha\\,\\nabla_\\theta L_{\\text{clip}}. \\\\[1mm]\n",
    "\\quad \\textbf{End For (mini-batch)} \\\\[1mm]\n",
    "\\textbf{End For (epoch)} \\\\[1mm]\n",
    "\\textbf{Return } \\theta.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{Algorithm: Offline PPO for LLM Fine-Tuning with RLHF} \\\\[1mm]\n",
    "\\textbf{Input:} \\; \\mathcal{D} = \\{x\\} \\quad \\text{(a set of prompts)}\\\\[1mm]\n",
    "\\quad \\pi_{\\text{old}} \\quad \\text{(frozen reference policy, from SFT)}\\\\[1mm]\n",
    "\\quad \\pi_\\theta \\quad \\text{(current policy to be updated)}\\\\[1mm]\n",
    "\\quad r_\\psi(x,y) \\quad \\text{(reward model trained from human feedback)}\\\\[1mm]\n",
    "\\quad V_\\phi(x) \\quad \\text{(value function approximator; optional)}\\\\[1mm]\n",
    "\\quad \\alpha, \\; \\epsilon, \\; \\beta, \\; E, \\; B \\quad \\text{(learning rate, clip parameter, KL coefficient, epochs, batch size)} \\\\[2mm]\n",
    "\\textbf{For } \\text{epoch} = 1 \\text{ to } E \\textbf{ do:} \\\\[1mm]\n",
    "\\quad \\text{Shuffle } \\mathcal{D} \\text{ and partition into mini-batches of size } B. \\\\[1mm]\n",
    "\\quad \\textbf{For each mini-batch } \\{x_i\\}_{i=1}^B \\textbf{ do:} \\\\[1mm]\n",
    "\\qquad \\textbf{For each prompt } x \\text{ in the mini-batch do:} \\\\[1mm]\n",
    "\\qquad\\quad \\text{Generate output } y \\sim \\pi_\\theta(\\cdot \\mid x) \\\\[1mm]\n",
    "\\qquad\\quad \\text{Compute } L_{\\text{new}}(x,y)=\\log \\pi_\\theta(y \\mid x) \\\\[1mm]\n",
    "\\qquad\\quad \\text{Compute } L_{\\text{old}}(x,y)=\\log \\pi_{\\text{old}}(y \\mid x) \\\\[1mm]\n",
    "\\qquad\\quad \\text{Compute the importance ratio: } r(x,y)=\\exp\\Bigl(L_{\\text{new}}(x,y)-L_{\\text{old}}(x,y)\\Bigr) \\\\[1mm]\n",
    "\\qquad\\quad \\text{Obtain reward } R(x,y)=r_\\psi(x,y) \\\\[1mm]\n",
    "\\qquad\\quad \\text{Compute baseline value } V(x)=V_\\phi(x) \\quad \\text{(or use a fixed baseline)} \\\\[1mm]\n",
    "\\qquad\\quad \\text{Compute advantage: } A(x,y)=R(x,y)-V(x) \\\\[1mm]\n",
    "\\qquad \\textbf{End For} \\\\[1mm]\n",
    "\\qquad \\text{Compute surrogate objectives:} \\\\[1mm]\n",
    "\\qquad\\quad L_1 = r(x,y) \\, A(x,y) \\\\[1mm]\n",
    "\\qquad\\quad L_2 = \\operatorname{clip}\\Bigl(r(x,y),\\, 1-\\epsilon,\\, 1+\\epsilon\\Bigr) \\, A(x,y) \\\\[1mm]\n",
    "\\qquad\\quad L_{\\text{clip}} = \\frac{1}{B}\\sum_{(x,y)\\in \\text{batch}} \\min\\{L_1, L_2\\} \\\\[1mm]\n",
    "\\qquad \\text{Optionally, compute value loss: } L_{\\text{value}} = \\frac{1}{B}\\sum_{(x,y)} \\Bigl(V_\\phi(x) - R(x,y)\\Bigr)^2 \\\\[1mm]\n",
    "\\qquad \\text{Optionally, compute entropy bonus: } \\mathcal{H} = \\frac{1}{B}\\sum_{(x,y)} \\mathcal{H}\\Bigl(\\pi_\\theta(\\cdot \\mid x)\\Bigr) \\\\[1mm]\n",
    "\\qquad \\text{Optionally, compute KL divergence: } \\text{KL} = \\frac{1}{B}\\sum_{(x,y)} D_{\\mathrm{KL}}\\Bigl(\\pi_{\\text{old}}(\\cdot \\mid x) \\, \\| \\, \\pi_\\theta(\\cdot \\mid x)\\Bigr) \\\\[1mm]\n",
    "\\qquad \\text{Total loss: } L_{\\text{total}} = L_{\\text{clip}} + c_1 \\, L_{\\text{value}} - c_2 \\, \\mathcal{H} + \\beta\\,\\text{KL} \\\\[1mm]\n",
    "\\qquad \\text{Update } \\theta \\text{ and (if applicable) } \\phi \\text{ via gradient descent on } L_{\\text{total}}. \\\\[1mm]\n",
    "\\quad \\textbf{End For (mini-batch)} \\\\[1mm]\n",
    "\\textbf{End For (epoch)} \\\\[2mm]\n",
    "\\textbf{Output: } \\pi_\\theta \\quad \\text{(the fine-tuned policy)}.\n",
    "\\end{array}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepstate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
