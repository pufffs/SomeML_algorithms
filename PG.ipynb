{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient Theorem Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is: \n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\sum_{t=0}^{\\infty} \\gamma^t\\,r(s_t, a_t)\\right]\n",
    "$$\n",
    "\n",
    "We can write:\n",
    "$$\n",
    "J(\\theta) = \\int p_\\theta(\\tau)\\left[\\sum_{t=0}^{\\infty} \\gamma^t\\,r(s_t,a_t)\\right]\\,d\\tau.\n",
    "$$\n",
    "\n",
    "Taking the gradient with respect to $\\theta$:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\int p_\\theta(\\tau)\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right]d\\tau.\n",
    "$$\n",
    "Assuming we can interchange the gradient and the integral:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int \\nabla_\\theta p_\\theta(\\tau)\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right]d\\tau.\n",
    "$$\n",
    "\n",
    "Applying the Log-Likelihood trick gives:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int p_\\theta(\\tau)\\,\\nabla_\\theta \\log p_\\theta(\\tau) \\left[\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right]d\\tau.\n",
    "$$\n",
    "In expectation notation:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\nabla_\\theta \\log p_\\theta(\\tau) \\,\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right].\n",
    "$$\n",
    "\n",
    "\n",
    "Since the trajectory probability factorizes as\n",
    "$$\n",
    "p_\\theta(\\tau) = p(s_0)\\prod_{t=0}^{\\infty} \\left[\\pi_\\theta(a_t \\mid s_t) \\,P(s_{t+1} \\mid s_t,a_t)\\right],\n",
    "$$\n",
    "its logarithm is\n",
    "$$\n",
    "\\log p_\\theta(\\tau) = \\log p(s_0) + \\sum_{t=0}^{\\infty}\\Bigl[\\log \\pi_\\theta(a_t \\mid s_t) + \\log P(s_{t+1} \\mid s_t,a_t)\\Bigr].\n",
    "$$\n",
    "Only the terms $\\log \\pi_\\theta(a_t \\mid s_t)$ depend on $\\theta$. Therefore,\n",
    "$$\n",
    "\\nabla_\\theta \\log p_\\theta(\\tau) = \\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t).\n",
    "$$\n",
    "Substitute back into our gradient expression:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\left(\\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\right)\\left(\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right)\\right].\n",
    "$$\n",
    "\n",
    "For each time step t, decompose the total return as:\n",
    "$$\n",
    "\\sum_{m=0}^{\\infty}\\gamma^m\\,r(s_m,a_m)\n",
    "=\\underbrace{\\sum_{m=0}^{t-1}\\gamma^m\\,r(s_m,a_m)}_{\\text{past (independent of \\(a_t\\))}}\n",
    "+\\underbrace{\\sum_{m=t}^{\\infty}\\gamma^m\\,r(s_m,a_m)}_{\\text{future (dependent on \\(a_t\\))}}.\n",
    "$$\n",
    "Since the past portion is independent of $(a_t)$, its contribution to $(\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t))$ vanishes in expectation. Hence, we have:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\left(\\sum_{m=t}^{\\infty}\\gamma^m\\,r(s_m,a_m)\\right)\\right].\n",
    "$$\n",
    "\n",
    "Since the Q function is:\n",
    "$$\n",
    "Q^\\pi(s_t,a_t) = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty}\\gamma^k\\,r(s_{t+k},a_{t+k}) \\,\\Bigm|\\, s_t,a_t\\right].\n",
    "$$\n",
    "Thus, the partial sum $(\\sum_{m=t}^{\\infty}\\gamma^m\\,r(s_m,a_m)$ is an unbiased sample of $(Q^\\pi(s_t,a_t))$. Therefore,\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\,Q^\\pi(s_t,a_t)\\right].\n",
    "$$\n",
    "\n",
    "\n",
    "Each trajectory $(\\tau)$ consists of a sequence of state–action pairs $((s_0,a_0), (s_1,a_1), \\ldots)$. The sum over time steps is equivalent to taking an expectation with respect to the **discounted occupancy measure** $(\\mu_\\pi(s,a))$, which represents the (normalized) distribution of state–action pairs visited by $(\\pi_\\theta)$. Thus, we can write:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s,a) \\sim \\mu_\\pi}\\left[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)\\,Q^\\pi(s,a)\\right].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Final Policy Gradient Theorem**\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s,a) \\sim \\mu_\\pi}\\left[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)\\,Q^\\pi(s,a)\\right].\n",
    "}\n",
    "$$\n",
    "A common variance-reduced version uses the advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$:\n",
    "$$\n",
    "\\boxed{\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s,a) \\sim \\mu_\\pi}\\left[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)\\,A^\\pi(s,a)\\right].\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------------------\n",
    "# Environment: CartPole-v1\n",
    "# ---------------------\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]  # 4 for CartPole\n",
    "action_dim = env.action_space.n \n",
    "\n",
    "def train_online(agent, env, episodes=500, method='REINFORCE'):\n",
    "    rewards_per_episode = []\n",
    "    \n",
    "    # Set update frequency based on method\n",
    "    if method == 'REINFORCE':\n",
    "        update_freq = 'episode'\n",
    "    elif method == 'A2C':\n",
    "        update_freq = 'n_steps'\n",
    "        n_steps = 5  # Update every 5 steps\n",
    "    elif method == 'PPO':\n",
    "        update_freq = 'n_episodes'\n",
    "        n_episodes = 5  # Update after 5 episodes\n",
    "    elif method in ['NPG', 'TRPO']:\n",
    "        update_freq = 'episode'\n",
    "    \n",
    "    # Initialize buffers for different update frequencies\n",
    "    buffer = []\n",
    "    steps_since_update = 0\n",
    "    episodes_since_update = 0\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_traj = []\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if method in ['REINFORCE', 'NPG', 'TRPO']:\n",
    "                action, log_prob = agent.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                transition = (state, action, log_prob, reward, None, next_state, done or truncated)\n",
    "            elif method in ['A2C', 'PPO']:\n",
    "                action, log_prob, value = agent.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                transition = (state, action, log_prob, reward, value, next_state, done or truncated)\n",
    "            \n",
    "            episode_traj.append(transition)\n",
    "            buffer.append(transition)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Update based on frequency\n",
    "            if update_freq == 'n_steps':\n",
    "                steps_since_update += 1\n",
    "                if steps_since_update >= n_steps or done:\n",
    "                    if method == 'A2C':\n",
    "                        agent.update_td(buffer)  # Use TD targets\n",
    "                    buffer = []\n",
    "                    steps_since_update = 0\n",
    "        \n",
    "        rewards_per_episode.append(total_reward)\n",
    "        \n",
    "        # Update after episode if that's the chosen frequency\n",
    "        if update_freq == 'episode':\n",
    "            if method == 'REINFORCE':\n",
    "                rewards = [t[3] for t in episode_traj]\n",
    "                log_probs = [t[2] for t in episode_traj]\n",
    "                agent.update(rewards, log_probs)\n",
    "            elif method in ['NPG', 'TRPO']:\n",
    "                # For NPG/TRPO, we might need more data\n",
    "                data = [episode_traj]\n",
    "                rewards = [t[3] for t in episode_traj]\n",
    "                log_probs = [t[2] for t in episode_traj]\n",
    "                if method == 'NPG':\n",
    "                    agent.update(rewards, log_probs, data)\n",
    "                else:  # TRPO\n",
    "                    old_log_probs = [[t[2] for t in episode_traj]]\n",
    "                    agent.update([episode_traj], old_log_probs)\n",
    "        \n",
    "        # Update after N episodes for methods like PPO\n",
    "        if update_freq == 'n_episodes':\n",
    "            episodes_since_update += 1\n",
    "            if episodes_since_update >= n_episodes:\n",
    "                if method == 'PPO':\n",
    "                    agent.update_td(buffer)  # Use TD targets\n",
    "                buffer = []\n",
    "                episodes_since_update = 0\n",
    "        \n",
    "        if (ep+1) % 50 == 0:\n",
    "            print(f\"Online Episode {ep+1}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    return rewards_per_episode\n",
    "\n",
    "def collect_trajectories(agent, env, num_episodes=50, method='REINFORCE'):\n",
    "    trajectories = []\n",
    "    for ep in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        traj = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            if method in ['REINFORCE', 'NPG', 'TRPO']:\n",
    "                action, log_prob = agent.select_action(state)\n",
    "                # For actor-critic based methods, we may get additional info.\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                traj.append((state, action, log_prob, reward, None, next_state, done or truncated))\n",
    "            elif method in ['A2C', 'PPO']:\n",
    "                action, log_prob, value = agent.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                traj.append((state, action, log_prob, reward, value, next_state, done or truncated))\n",
    "            state = next_state\n",
    "        trajectories.append(traj)\n",
    "    return trajectories\n",
    "\n",
    "def train_offline(agent, offline_data, epochs=10, method='REINFORCE'):\n",
    "    # offline_data is a list of trajectories pre-collected\n",
    "    rewards_list = []\n",
    "    for epoch in range(epochs):\n",
    "        total_reward = 0\n",
    "        for traj in offline_data:\n",
    "            # For offline REINFORCE, we compute the update over each trajectory in the dataset.\n",
    "            if method == 'REINFORCE':\n",
    "                rewards = [entry[3] for entry in traj]\n",
    "                log_probs = [entry[2] for entry in traj]\n",
    "                total_reward += sum(rewards)\n",
    "                agent.update(rewards, log_probs)\n",
    "            elif method == 'A2C':\n",
    "                total_reward += sum([entry[3] for entry in traj])\n",
    "                agent.update(traj)\n",
    "            elif method == 'NPG':\n",
    "                rewards = [entry[3] for entry in traj]\n",
    "                log_probs = [entry[2] for entry in traj]\n",
    "                data = offline_data  # use entire offline dataset\n",
    "                agent.update(rewards, log_probs, data)\n",
    "            elif method == 'TRPO':\n",
    "                old_log_probs = [ [entry[2] for entry in traj] ]\n",
    "                agent.update([traj], old_log_probs)\n",
    "                total_reward += sum([entry[3] for entry in traj])\n",
    "            elif method == 'PPO':\n",
    "                agent.update([traj])\n",
    "                total_reward += sum([entry[3] for entry in traj])\n",
    "        rewards_list.append(total_reward / len(offline_data))\n",
    "        print(f\"Offline Epoch {epoch+1}, Average Total Reward per Trajectory: {rewards_list[-1]:.2f}\")\n",
    "    return rewards_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Objective:**\n",
    "\n",
    "The expected return is given by\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T-1}\\gamma^t\\,r(s_t,a_t)\\right],\n",
    "$$\n",
    "\n",
    "where $ \\tau = (s_0,a_0,\\dots,s_{T-1},a_{T-1}) $ is a complete episode.\n",
    "\n",
    "**Policy Gradient Theorem (using Monte Carlo return):**\n",
    "\n",
    "The gradient is\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) \\, G_t\\right],\n",
    "$$\n",
    "\n",
    "with the return\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{k=t}^{T-1}\\gamma^{k-t}\\,r(s_k,a_k).\n",
    "$$\n",
    "\n",
    "**Update Rule:**\n",
    "\n",
    "For each episode, update\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T-1} G_t \\, \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Policy network for REINFORCE (outputs action probabilities)\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc_policy = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.ReLU(self.fc1(x))\n",
    "        logits = self.fc_policy(x)\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "\n",
    "class REINFORCEAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-2, gamma=0.99):\n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.policy_net(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "    \n",
    "    def update(self, rewards, log_probs):\n",
    "        # Compute returns G_t = r_t + gamma*r_{t+1} + ... for each time step t.\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns, dtype=torch.float).to(device) #[r_0,r_0+r_1*\\gamma,r_0+r_1*\\gamma+r_2*\\gamma^2,...]\n",
    "        # Optional normalization for stability:\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        loss = 0\n",
    "        for log_prob, G in zip(log_probs, returns):\n",
    "            loss -= log_prob * G  # Note: gradient ascent on expected return.\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = REINFORCEAgent(state_dim, action_dim) \n",
    "online_rewards = train_online(agent, env, episodes=500, method=\"REINFORCE\")\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(online_rewards, label=\"Online Rewards\")\n",
    "plt.xlabel(\"Episode / Epoch\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Critic (A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture:**\n",
    "\n",
    "We have an actor (policy) and a critic (value function). The critic estimates\n",
    "\n",
    "$$\n",
    "V^\\pi(s) \\approx \\mathbb{E}\\left[G_t \\mid s_t = s\\right].\n",
    "$$\n",
    "\n",
    "**Advantage Estimate:**\n",
    "\n",
    "A common choice is the one-step temporal difference (TD) error:\n",
    "\n",
    "$$\n",
    "A_t = r(s_t,a_t) + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t).\n",
    "$$\n",
    "\n",
    "**Loss Functions:**\n",
    "\n",
    "- **Actor Loss:**\n",
    "\n",
    "$$\n",
    "L_{\\text{actor}}(\\theta) = -\\mathbb{E}\\left[\\log \\pi_\\theta(a_t\\mid s_t)\\,A_t\\right].\n",
    "$$\n",
    "\n",
    "- **Critic Loss:**\n",
    "\n",
    "$$\n",
    "L_{\\text{critic}}(\\phi) = \\frac{1}{2}\\mathbb{E}\\left[\\left(r(s_t,a_t) + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t)\\right)^2\\right].\n",
    "$$\n",
    "\n",
    "**Update Rules:**\n",
    "\n",
    "- **Actor Update:**\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha\\,\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) \\, A_t.\n",
    "$$\n",
    "\n",
    "- **Critic Update:**\n",
    "\n",
    "$$\n",
    "\\phi \\leftarrow \\phi - \\beta\\,\\nabla_\\phi L_{\\text{critic}}(\\phi).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic network for A2C/PPO (shared encoder, two heads: policy & value)\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc_policy = nn.Linear(hidden_dim, action_dim)\n",
    "        self.fc_value = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.torch.nn.ReLU(self.fc1(x))\n",
    "        policy_logits = self.fc_policy(x)\n",
    "        policy = torch.softmax(policy_logits, dim=-1)\n",
    "        value = self.fc_value(x)\n",
    "        return policy, value\n",
    "\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_dim, action_dim, actor_lr=1e-3, critic_lr=1e-3, gamma=0.99):\n",
    "        self.ac_net = ActorCriticNetwork(state_dim, action_dim).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.ac_net.fc_policy.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.ac_net.fc_value.parameters(), lr=critic_lr)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        policy, value = self.ac_net(state)\n",
    "        m = Categorical(policy)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action), value\n",
    "\n",
    "    def update(self, trajectory):\n",
    "        # trajectory: list of (state, action, log_prob, reward, value, next_state, done)\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for (_, _, _, reward, _, _, _) in reversed(trajectory):\n",
    "            R = reward + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, dtype=torch.float).to(device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        for (s, a, log_prob, reward, value, s_next, done), R in zip(trajectory, returns):\n",
    "            advantage = R - value.item()\n",
    "            policy_losses.append(-log_prob * advantage)\n",
    "            value_losses.append(nn.functional.mse_loss(value, torch.tensor([R], dtype=torch.float).to(device)))\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss = torch.stack(policy_losses).sum()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss = torch.stack(value_losses).sum()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "    # Add this method to the A2CAgent class\n",
    "    def update_td(self, transitions):\n",
    "        # Extract all data from transitions into tensors\n",
    "        states = torch.FloatTensor([t[0] for t in transitions]).to(device)\n",
    "        actions = torch.LongTensor([t[1] for t in transitions]).to(device)\n",
    "        log_probs = torch.stack([t[2] for t in transitions]).to(device)\n",
    "        rewards = torch.FloatTensor([t[3] for t in transitions]).to(device)\n",
    "        values = torch.stack([t[4] for t in transitions]).to(device).squeeze()\n",
    "        next_states = torch.FloatTensor([t[5] for t in transitions]).to(device)\n",
    "        dones = torch.FloatTensor([float(t[6]) for t in transitions]).to(device)\n",
    "        \n",
    "        # Calculate next state values for TD targets\n",
    "        with torch.no_grad():\n",
    "            _, next_values = self.ac_net(next_states)\n",
    "            next_values = next_values.squeeze()\n",
    "            next_values = next_values * (1 - dones)\n",
    "        \n",
    "        # Calculate TD targets: r + gamma * V(s')\n",
    "        td_targets = rewards + self.gamma * next_values\n",
    "        \n",
    "        # Calculate advantages: TD error\n",
    "        advantages = td_targets - values.detach()\n",
    "        \n",
    "        # Optional: Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Policy loss using advantages\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        # Value loss using TD targets\n",
    "        value_loss = nn.functional.mse_loss(values, td_targets)\n",
    "        \n",
    "        # Update actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward(retain_graph=True)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.critic_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Policy Gradient (NPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Idea:**\n",
    "\n",
    "The standard gradient is preconditioned by the inverse Fisher information matrix $F(\\theta) $ to obtain the natural gradient:\n",
    "\n",
    "$$\n",
    "\\Delta \\theta = F(\\theta)^{-1} \\, \\nabla_\\theta J(\\theta),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "F(\\theta) = \\mathbb{E}_{s \\sim d^\\pi,\\, a \\sim \\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s) \\, \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)^\\top\\right].\n",
    "$$\n",
    "\n",
    "**Update Rule:**\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\, \\Delta \\theta = \\theta + \\alpha \\, F(\\theta)^{-1} \\, \\nabla_\\theta J(\\theta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-2, gamma=0.99):\n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)  # Base optimizer (we will precondition manually)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.policy_net(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "    \n",
    "    def compute_gradient(self, rewards, log_probs):\n",
    "        # Similar to REINFORCE, compute the gradient vector.\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns, dtype=torch.float).to(device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        grad_vector = []\n",
    "        for log_prob, G in zip(log_probs, returns):\n",
    "            grad_vector.append(G * log_prob)\n",
    "        # Sum gradients (each log_prob is a scalar multiplied by the gradient vector of the policy network)\n",
    "        # We compute the gradient of the loss with respect to parameters manually.\n",
    "        loss = -torch.stack(grad_vector).sum()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gather the gradient vector from all parameters into a single flattened vector.\n",
    "        grad_flat = torch.cat([p.grad.view(-1) for p in self.policy_net.parameters()])\n",
    "        return grad_flat.detach()\n",
    "    \n",
    "    def estimate_fisher(self, data, num_samples=10):\n",
    "        # Estimate the Fisher Information Matrix from collected data.\n",
    "        grads = []\n",
    "        for _ in range(num_samples):\n",
    "            # Sample one trajectory from data (offline dataset) or one rollout\n",
    "            traj = data[np.random.randint(len(data))]\n",
    "            # For simplicity, use the first time step.\n",
    "            s, a, _, _, _, _, _ = traj[0]\n",
    "            s = torch.FloatTensor(s).unsqueeze(0).to(device)\n",
    "            probs = self.policy_net(s)\n",
    "            m = Categorical(probs)\n",
    "            log_prob = m.log_prob(torch.tensor(a).to(device))\n",
    "            self.policy_net.zero_grad()\n",
    "            log_prob.backward()\n",
    "            grad_sample = torch.cat([p.grad.view(-1) for p in self.policy_net.parameters()])\n",
    "            grads.append(grad_sample.unsqueeze(0))\n",
    "        grads = torch.cat(grads, dim=0)  # [num_samples, D]\n",
    "        # Fisher is approximated by the outer product of the gradients\n",
    "        F = torch.matmul(grads.t(), grads) / num_samples\n",
    "        return F.detach()\n",
    "    \n",
    "    def update(self, rewards, log_probs, data):\n",
    "        # Compute regular REINFORCE gradient:\n",
    "        g = self.compute_gradient(rewards, log_probs)\n",
    "        # Estimate Fisher matrix using some offline data:\n",
    "        F = self.estimate_fisher(data)\n",
    "        # Solve for natural gradient: Δθ = F^{-1}g\n",
    "        # For simplicity, we use torch.linalg.solve (assuming F is invertible)\n",
    "        natural_grad = torch.linalg.solve(F, g)\n",
    "        # Update parameters manually:\n",
    "        index = 0\n",
    "        for p in self.policy_net.parameters():\n",
    "            numel = p.numel()\n",
    "            p.data += 1e-2 * natural_grad[index:index+numel].view_as(p)\n",
    "            index += numel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust Region Policy Optimization (TRPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of reinforcement learning is to maximize the expected return:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]=\\eta(\\pi_{\\theta})\n",
    "$$\n",
    "Consider the expected return under the new policy $\\pi_{\\theta'}$:\n",
    "\n",
    "$$\n",
    "J(\\theta') = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta'}}[R(\\tau)]\n",
    "$$\n",
    "\n",
    "Using importance sampling with respect to the old policy $\\pi_{\\theta_{\\text{old}}}$, we rewrite:\n",
    "\n",
    "$$\n",
    "J(\\theta') = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\frac{\\pi_{\\theta'}(\\tau)}{\\pi_{\\theta_{\\text{old}}}(\\tau)}R(\\tau)\\right]\n",
    "$$\n",
    "\n",
    "However, directly computing $\\frac{\\pi_{\\theta'}(\\tau)}{\\pi_{\\theta_{\\text{old}}}(\\tau)}$ over entire trajectories is challenging. Therefore, TRPO approximates this by using single-step importance sampling and the advantage function $A^{\\pi_{\\theta_{\\text{old}}}}(s,a)$:\n",
    "\n",
    "$$\n",
    "J(\\theta') \\approx \\eta(\\pi_{\\theta_{\\text{old}}}) + \\sum_s d^{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_a \\pi_{\\theta'}(a|s)A^{\\pi_{\\theta_{\\text{old}}}}(s,a)\n",
    "$$\n",
    "\n",
    "Here, we define the surrogate objective clearly as:\n",
    "\n",
    "$$\n",
    "L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) = \\eta(\\pi_{\\theta_{\\text{old}}}) + \\sum_s d^{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_a \\pi_{\\theta'}(a|s)A^{\\pi_{\\theta_{\\text{old}}}}(s,a)\n",
    "$$\n",
    "\n",
    "Recall the **Conservative Policy Iteration (CPI)** lower bound:\n",
    "$$\n",
    "L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) \\geq \\eta(\\pi_{\\theta_{\\text{old}}}) + \\sum_{s} d^{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_{a}\\pi_{\\theta'}(a|s)A^{\\pi_{\\theta_{\\text{old}}}}(s,a) - \\frac{2\\gamma\\epsilon}{(1-\\gamma)^2} D_{\\text{KL}}^{\\text{max}}(\\pi_{\\theta_{\\text{old}}}\\|\\pi_{\\theta'})\n",
    "$$\n",
    "The CPI bound above is theoretically insightful but practically restrictive due to the term $D_{\\text{KL}}^{\\text{max}}(\\pi||\\pi')$, which requires bounding the divergence at **every state**. \n",
    "\n",
    "In practice, we replace the \"max KL divergence\" with an **average KL divergence** to simplify computation:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}^{\\text{max}}(\\pi_{\\theta_{\\text{old}}}\\|\\pi_{\\theta'}) \\approx \\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)\\|\\pi_{\\theta'}(\\cdot|s))]\n",
    "$$\n",
    "\n",
    "This leads to a more manageable lower bound:\n",
    "\n",
    "$$\n",
    "L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) \\geq \\eta(\\pi_{\\theta_{\\text{old}}}) + \\sum_{s} d^{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_{a}\\pi_{\\theta'}(a|s)A^{\\pi_{\\theta_{\\text{old}}}}(s,a) - \\frac{2\\gamma\\epsilon}{(1-\\gamma)^2}\\cdot\\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)\\|\\pi_{\\theta'}(\\cdot|s))]\n",
    "$$\n",
    "To ensure policy improvement, we aim to maximize the lower bound above. Equivalently, we can pose this as a constrained optimization problem. Specifically, we seek a new policy $\\pi'$ that maximizes (since $\\eta(\\pi_{\\theta_{\\text{old}}})$ is constant):\n",
    "\n",
    "$$\n",
    "L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) = \\sum_{s} d^{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_{a}\\pi_{\\theta'}(a|s)A^{\\pi_{\\theta_{\\text{old}}}}(s,a)=\\mathbb{E}_{s,a \\sim \\pi_{\\theta_{\\text{old}}}} \\left[ \\frac{\\pi_{\\theta'}(a|s)}{\\pi_{\\theta_{old}}(a|s)}A^{\\pi_{\\theta_{\\text{old}}}}(s,a)\\right]\n",
    "$$\n",
    "\n",
    "subject to a constraint on the KL divergence between the old policy and the new policy:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)\\|\\pi_{\\theta'}(\\cdot|s))] \\leq \\delta\n",
    "$$\n",
    "\n",
    "Here, $\\delta$ is a hyperparameter chosen to control how aggressively the policy can change in each update.\n",
    "\n",
    "Thus, the optimization becomes explicitly:\n",
    "\n",
    "$$\n",
    "\\max_{\\pi_{\\theta'}} L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) \\quad\\text{s.t.}\\quad \\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)\\|\\pi_{\\theta'}(\\cdot|s))] \\leq \\delta\n",
    "$$\n",
    "\n",
    "We approximate the surrogate objective around $\\theta_{\\text{old}}$ with a first-order (linear) Taylor expansion:\n",
    "\n",
    "$$\n",
    "L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) \\approx L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta_{\\text{old}}}) + \\nabla_{\\theta} L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta})_{\\theta =\\theta_{old}}^\\top (\\theta' - \\theta_{\\text{old}})\n",
    "$$\n",
    "\n",
    "For the KL divergence constraint, we apply a second-order (quadratic) Taylor expansion around $\\theta_{\\text{old}}$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}}\\left[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)||\\pi_{\\theta'}(\\cdot|s))\\right] \\approx \\frac{1}{2}(\\theta' - \\theta_{\\text{old}})^\\top F(\\theta_{\\text{old}})(\\theta' - \\theta_{\\text{old}})\n",
    "$$\n",
    "\n",
    "where the Fisher Information Matrix (FIM), $F(\\theta_{\\text{old}})$, is defined as:\n",
    "\n",
    "$$\n",
    "F(\\theta_{\\text{old}}) = \\mathbb{E}_{s,a \\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^\\top\\right]\\Big|_{\\theta=\\theta_{\\text{old}}}\n",
    "$$\n",
    "\n",
    "Combining the above approximations yields a simpler constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\max_{\\theta'} \\nabla_{\\theta} L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta})_{\\theta =\\theta_{old}}^\\top(\\theta' - \\theta_{\\text{old}}) \\quad \\text{subject to} \\quad \\frac{1}{2}(\\theta' - \\theta_{\\text{old}})^\\top F(\\theta_{\\text{old}})(\\theta' - \\theta_{\\text{old}}) \\leq \\delta\n",
    "$$\n",
    "\n",
    "Using the method of Lagrange multipliers, we derive the optimal policy update step explicitly as:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\sqrt{\\frac{2\\delta}{g^\\top F^{-1} g}} F^{-1} g\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $g = \\nabla_{\\theta} L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta})_{\\theta =\\theta_{old}}$ is the policy gradient evaluated at $\\theta_{old}$ by using the logrithm trick.\n",
    "- $F$ is the Fisher Information Matrix evaluated at $\\theta_{\\text{old}}$.\n",
    "\n",
    "In practice, directly computing the inverse $F^{-1}$ is computationally expensive. Thus, TRPO uses the conjugate gradient method to approximate the product $F^{-1} g$ efficiently without explicitly computing the inverse.\n",
    "\n",
    "---\n",
    "**TRPO Algorithm**\n",
    "\n",
    "**Given:** initial policy parameters $\\theta_0$, KL-divergence constraint parameter $\\delta$\n",
    "\n",
    "**for** iteration $k=0,1,2,\\dots$ **do**:\n",
    "\n",
    "1. **Collect trajectories** by executing policy $\\pi_{\\theta_k}(a|s)$.\n",
    "\n",
    "2. **Compute advantages** $A^{\\pi_{\\theta_k}}(s,a)$ using collected data.\n",
    "\n",
    "3. **Compute policy gradient:**\n",
    "$$\n",
    "g = \\mathbb{E}_{s,a \\sim \\pi_{\\theta_k}}\\left[\\nabla_{\\theta'}\\log\\pi_{\\theta'}(a|s)\\big|_{\\theta'=\\theta_k} A^{\\pi_{\\theta_k}}(s,a)\\right]\n",
    "$$\n",
    "\n",
    "4. **Estimate Fisher Information Matrix (FIM)**:\n",
    "$$\n",
    "F(\\theta_k) = \\mathbb{E}_{s,a \\sim \\pi_{\\theta_k}}\\left[\\nabla_{\\theta_k}\\log\\pi_{\\theta_k}(a|s)\\nabla_{\\theta_k}\\log\\pi_{\\theta_k}(a|s)^\\top\\right]\n",
    "$$\n",
    "\n",
    "5. **Compute policy update direction** by approximately solving:\n",
    "$$\n",
    "F(\\theta_k)x = g\n",
    "$$\n",
    "using the **conjugate gradient method**.\n",
    "\n",
    "6. **Update policy parameters**:\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k + \\sqrt{\\frac{2\\delta}{g^\\top x}}\\,x\n",
    "$$\n",
    "\n",
    "**end for**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simplified TRPO\n",
    "class TRPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, max_kl=1e-2):\n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.gamma = gamma\n",
    "        self.max_kl = max_kl\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.policy_net(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "    \n",
    "    def compute_loss_and_kl(self, trajectories, old_log_probs):\n",
    "        # trajectories: list of episodes (each episode: list of (s, a, log_prob, r, ...))\n",
    "        # Compute surrogate loss and mean KL divergence between old and new policies.\n",
    "        all_loss = []\n",
    "        all_kl = []\n",
    "        for traj, old_lp in zip(trajectories, old_log_probs):\n",
    "            returns = []\n",
    "            G = 0\n",
    "            for (_, _, _, r, _, _, _) in reversed(traj):\n",
    "                G = r + self.gamma * G\n",
    "                returns.insert(0, G)\n",
    "            returns = torch.tensor(returns, dtype=torch.float).to(device)\n",
    "            # Normalize returns:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "            for (s, a, _, r, _, _, _), old_logp in zip(traj, old_lp):\n",
    "                s = torch.FloatTensor(s).unsqueeze(0).to(device)\n",
    "                probs = self.policy_net(s)\n",
    "                m = Categorical(probs)\n",
    "                new_logp = m.log_prob(torch.tensor(a).to(device))\n",
    "                ratio = torch.exp(new_logp - old_logp)\n",
    "                # Here we use the full return as an estimator of Q\n",
    "                loss = -ratio * returns[0]  # simplified; normally use advantage\n",
    "                all_loss.append(loss)\n",
    "                kl = torch.distributions.kl_divergence(\n",
    "                    Categorical(probs_old), m)  # we need old probs\n",
    "                all_kl.append(kl)\n",
    "        loss_mean = torch.stack(all_loss).mean()\n",
    "        kl_mean = torch.stack(all_kl).mean()\n",
    "        return loss_mean, kl_mean\n",
    "\n",
    "    def update(self, trajectories, old_log_probs):\n",
    "        # TRPO update: Solve constrained optimization problem.\n",
    "        # In our simplified version, we just show the outline.\n",
    "        # A full implementation requires computing the natural gradient via conjugate gradient\n",
    "        # and then performing a line search to enforce KL constraint.\n",
    "        loss, kl = self.compute_loss_and_kl(trajectories, old_log_probs)\n",
    "        # Here we would compute the natural gradient and then update parameters while checking KL.\n",
    "        # For demonstration, we simply take a small gradient step if kl < max_kl.\n",
    "        self.policy_net.zero_grad()\n",
    "        loss.backward()\n",
    "        # Compute mean KL divergence (placeholder; normally computed during conjugate gradient)\n",
    "        if kl < self.max_kl:\n",
    "            for p in self.policy_net.parameters():\n",
    "                p.data -= 1e-2 * p.grad  # note the minus sign since loss is negative surrogate.\n",
    "        # In a real TRPO implementation, we would use a second-order method.\n",
    "        \n",
    "\n",
    "class TRPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, max_kl=1e-2):\n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        # Add a value network for TD targets\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        ).to(device)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=1e-3)\n",
    "        self.gamma = gamma\n",
    "        self.max_kl = max_kl\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.policy_net(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        # Also return value estimate\n",
    "        value = self.value_net(state)\n",
    "        return action.item(), m.log_prob(action), value\n",
    "\n",
    "    def compute_loss_and_kl_td(self, trajectories, old_log_probs):\n",
    "        # Compute surrogate loss and KL using TD targets instead of Monte Carlo returns\n",
    "        all_loss = []\n",
    "        all_kl = []\n",
    "        all_value_loss = []\n",
    "        \n",
    "        for traj, old_lp in zip(trajectories, old_log_probs):\n",
    "            states = torch.FloatTensor([t[0] for t in traj]).to(device)\n",
    "            actions = torch.LongTensor([t[1] for t in traj]).to(device)\n",
    "            rewards = torch.FloatTensor([t[3] for t in traj]).to(device)\n",
    "            next_states = torch.FloatTensor([t[5] for t in traj]).to(device)\n",
    "            dones = torch.FloatTensor([float(t[6]) for t in traj]).to(device)\n",
    "            \n",
    "            # Get current values and old probs\n",
    "            current_values = self.value_net(states).squeeze()\n",
    "            \n",
    "            # Calculate TD targets with torch.no_grad()\n",
    "            with torch.no_grad():\n",
    "                next_values = self.value_net(next_states).squeeze()\n",
    "                next_values = next_values * (1 - dones)\n",
    "                td_targets = rewards + self.gamma * next_values\n",
    "                # Calculate advantages and detach from computational graph\n",
    "                advantages = (td_targets - current_values).detach()\n",
    "                # Normalize advantages\n",
    "                if len(advantages) > 1:  # Only normalize if we have enough samples\n",
    "                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "            \n",
    "            # Compute value loss\n",
    "            value_loss = nn.functional.mse_loss(current_values, td_targets)\n",
    "            all_value_loss.append(value_loss)\n",
    "            \n",
    "            # Compute policy loss and KL divergence\n",
    "            for i, ((s, a, _, _, _, _, _), old_logp) in enumerate(zip(traj, old_lp)):\n",
    "                s = torch.FloatTensor(s).unsqueeze(0).to(device)\n",
    "                a = torch.LongTensor([a]).to(device)\n",
    "                \n",
    "                # Get current policy\n",
    "                probs = self.policy_net(s)\n",
    "                m = Categorical(probs)\n",
    "                new_logp = m.log_prob(a)\n",
    "                \n",
    "                # Get old policy distribution for KL calculation\n",
    "                old_probs = torch.zeros_like(probs)  # Placeholder\n",
    "                # In practice, you would store the old policy distribution\n",
    "                # Here we're simplifying by assuming we have access to old_probs\n",
    "                old_m = Categorical(old_probs)\n",
    "                old_logp = old_m.log_prob(a)\n",
    "                \n",
    "                # Compute ratio and surrogate loss\n",
    "                ratio = torch.exp(new_logp - old_logp)\n",
    "                # Use the advantage from TD error\n",
    "                loss = -ratio * advantages[i]\n",
    "                all_loss.append(loss)\n",
    "                \n",
    "                # Compute KL divergence\n",
    "                kl = torch.distributions.kl_divergence(old_m, m)\n",
    "                all_kl.append(kl)\n",
    "        \n",
    "        # Average losses and KL\n",
    "        loss_mean = torch.stack(all_loss).mean()\n",
    "        kl_mean = torch.stack(all_kl).mean()\n",
    "        value_loss_mean = torch.stack(all_value_loss).mean()\n",
    "        \n",
    "        return loss_mean, kl_mean, value_loss_mean\n",
    "\n",
    "    def update(self, trajectories, old_log_probs):\n",
    "        # First update value network\n",
    "        self.value_optimizer.zero_grad()\n",
    "        _, _, value_loss = self.compute_loss_and_kl_td(trajectories, old_log_probs)\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        # Then update policy with TRPO\n",
    "        loss, kl, _ = self.compute_loss_and_kl_td(trajectories, old_log_probs)\n",
    "        \n",
    "        # In a full TRPO implementation:\n",
    "        # 1. Compute policy gradient\n",
    "        # 2. Compute Fisher Information Matrix\n",
    "        # 3. Solve Fx = g using conjugate gradient\n",
    "        # 4. Perform line search to ensure KL constraint\n",
    "        \n",
    "        # For our simplified version:\n",
    "        self.policy_net.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Only update if KL constraint is satisfied\n",
    "        if kl < self.max_kl:\n",
    "            for p in self.policy_net.parameters():\n",
    "                p.data -= 1e-2 * p.grad  # Note the minus sign for gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trust Region Policy Optimization (TRPO) solves the following constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\max_{\\theta'} L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) = \\mathbb{E}_{s,a\\sim\\pi_{\\theta_{\\text{old}}}}\\left[\\frac{\\pi_{\\theta'}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)}A^{\\pi_{\\theta_{\\text{old}}}}(s,a)\\right]\n",
    "$$\n",
    "\n",
    "subject to the KL-divergence constraint:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{s\\sim d^{\\pi_{\\theta_{\\text{old}}}}}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)||\\pi_{\\theta'}(\\cdot|s))]\\leq \\delta\n",
    "$$\n",
    "\n",
    "TRPO is computationally expensive due to the constraint on KL divergence. PPO simplifies TRPO by using an easier-to-compute penalty mechanism that achieves similar stability without explicitly enforcing KL constraints.\n",
    "\n",
    "Specifically, PPO introduces the following \"surrogate objective\" (without explicit constraints):\n",
    "\n",
    "$$\n",
    "L^{\\text{PPO}}(\\theta) = \\mathbb{E}_{s,a\\sim\\pi_{\\theta_{\\text{old}}}}\\left[\n",
    "r(\\theta)A^{\\pi_{\\theta_{\\text{old}}}}(s,a) - \\beta D_{\\text{KL}}\\bigl(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)||\\pi_{\\theta}(\\cdot|s)\\bigr)\n",
    "\\right]\n",
    "$$\n",
    "While the explicit KL penalty term in PPO is easy, tuning the coefficient $\\beta$ can be tricky. PPO further simplifies this by introducing a \"clipped\" surrogate objective:\n",
    "\n",
    "$$\n",
    "L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_{s,a\\sim\\pi_{\\theta_{\\text{old}}}}\\left[\n",
    "\\min\\left(\n",
    "r(\\theta)A^{\\pi_{\\theta_{\\text{old}}}}(s,a),\\,\\text{clip}(r(\\theta),1-\\epsilon,1+\\epsilon)A^{\\pi_{\\theta_{\\text{old}}}}(s,a)\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "This removes the need for explicitly adjusting a penalty coefficient $\\beta$, thus making PPO easier to implement and tune.\n",
    "\n",
    "---\n",
    "\n",
    "#### How PPO Clipping Acts as an Implicit Penalty\n",
    "\n",
    "Although PPO doesn’t explicitly add a penalty term like KL divergence, the clipping function implicitly penalizes overly large policy changes. Here's how:\n",
    "\n",
    "(1) When Advantage $A^{\\pi_{\\theta_k}}(s,a)>0$ (action better than average):\n",
    "\n",
    "- If $r(\\theta)$ is too large (> $1+\\epsilon$), the objective clips and doesn't reward further increases.\n",
    "- If $r(\\theta)$ is too small (<$1-\\epsilon$), it naturally reduces the objective since we're multiplying a smaller ratio with a positive advantage.\n",
    "\n",
    "(2) When Advantage $A^{\\pi_{\\theta_k}}(s,a)<0$ (action worse than average):\n",
    "\n",
    "- If $r(\\theta)$ is too small (<$1-\\epsilon$), it clips at $1-\\epsilon$, Now multiplying a negative advantage by a larger number ($1-\\epsilon$) creates a less-negative objective than the original ratio would, thus effectively limiting the \"benefit\" of aggressively reducing the probability of bad actions (but still allowing a controlled reduction).\n",
    "\n",
    "- If $r(\\theta)$ is becomes large (> $1+\\epsilon$) for negative advantages, PPO explicitly does not clip upwards. Here, the large ratio multiplied by negative advantage naturally yields a very negative objective value, strongly discouraging that undesirable policy update.\n",
    "\n",
    "\n",
    "Thus, clipping effectively serves as an implicit \"penalty\" mechanism, guiding the policy to update cautiously.\n",
    "\n",
    "---\n",
    "**PPO Algorithm**\n",
    "\n",
    "**Given:** Initial policy parameters $\\theta_0$, clipping parameter $\\epsilon$ (e.g., 0.2), learning rate $\\eta$\n",
    "\n",
    "**for** iteration $k = 0, 1, 2, \\dots$ **do**:\n",
    "\n",
    "1. Collect trajectories using policy $\\pi_{\\theta_k}(a|s)$.\n",
    "\n",
    "2. Compute advantage estimates $A^{\\pi_{\\theta_k}}(s,a)$.\n",
    "\n",
    "3. **for** each gradient update epoch **do**:\n",
    "\n",
    "   - Compute probability ratios:\n",
    "     $$\n",
    "     r(\\theta) = \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}\n",
    "     $$\n",
    "\n",
    "   - Maximize the clipped PPO objective:\n",
    "     $$\n",
    "     L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_{s,a\\sim \\pi_{\\theta_k}}\\left[\n",
    "     \\min\\left(r(\\theta)A^{\\pi_{\\theta_k}}(s,a),\\,\\text{clip}(r(\\theta),1-\\epsilon,1+\\epsilon)A^{\\pi_{\\theta_k}}(s,a)\\right)\n",
    "     \\right]\n",
    "     $$\n",
    "\n",
    "   by performing gradient ascent step:\n",
    "   $$\n",
    "   \\theta \\leftarrow \\theta + \\eta \\nabla_{\\theta} L^{\\text{CLIP}}(\\theta)\n",
    "   $$\n",
    "\n",
    "3. Update the old policy:\n",
    "   $$\n",
    "   \\theta_{k+1}\\leftarrow \\theta\n",
    "   $$\n",
    "\n",
    "**end for**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=3e-4, gamma=0.99, clip_epsilon=0.2, update_epochs=4):\n",
    "        self.ac_net = ActorCriticNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.ac_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.update_epochs = update_epochs\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        policy, value = self.ac_net(state)\n",
    "        m = Categorical(policy)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action), value\n",
    "\n",
    "    def compute_returns_and_advantages(self, trajectories):\n",
    "        all_returns = []\n",
    "        all_advantages = []\n",
    "        for traj in trajectories:\n",
    "            returns = []\n",
    "            G = 0\n",
    "            for (_, _, _, r, _, _, _) in reversed(traj):\n",
    "                G = r + self.gamma * G\n",
    "                returns.insert(0, G)\n",
    "            returns = torch.tensor(returns, dtype=torch.float).to(device)\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "            advantages = []\n",
    "            for (i, (_, _, _, _, value, _, _)) in enumerate(traj):\n",
    "                advantages.append(returns[i] - value.item())\n",
    "            all_returns.append(returns)\n",
    "            all_advantages.append(torch.tensor(advantages, dtype=torch.float).to(device))\n",
    "        return all_returns, all_advantages\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        states, actions, old_log_probs, returns, advantages = [], [], [], [], []\n",
    "        for traj in trajectories:\n",
    "            for (s, a, logp, r, v, s_next, done) in traj:\n",
    "                states.append(s)\n",
    "                actions.append(a)\n",
    "                old_log_probs.append(logp)\n",
    "            ret, adv = self.compute_returns_and_advantages([traj])\n",
    "            returns += ret.tolist()\n",
    "            advantages += adv.tolist()\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        old_log_probs = torch.stack(old_log_probs).to(device)\n",
    "        returns = torch.tensor(returns, dtype=torch.float).to(device)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float).to(device)\n",
    "        \n",
    "        for _ in range(self.update_epochs):\n",
    "            policy, values = self.ac_net(states)\n",
    "            m = Categorical(policy)\n",
    "            new_log_probs = m.log_prob(actions)\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = nn.functional.mse_loss(values.squeeze(), returns)\n",
    "            entropy_loss = -m.entropy().mean()\n",
    "            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "    # Add this method to the PPOAgent class\n",
    "    def update_td(self, transitions):\n",
    "        # Extract all data from transitions into tensors\n",
    "        states = torch.FloatTensor([t[0] for t in transitions]).to(device)\n",
    "        actions = torch.LongTensor([t[1] for t in transitions]).to(device)\n",
    "        old_log_probs = torch.stack([t[2] for t in transitions]).to(device)\n",
    "        rewards = torch.FloatTensor([t[3] for t in transitions]).to(device)\n",
    "        values = torch.stack([t[4] for t in transitions]).to(device).squeeze()\n",
    "        next_states = torch.FloatTensor([t[5] for t in transitions]).to(device)\n",
    "        dones = torch.FloatTensor([float(t[6]) for t in transitions]).to(device)\n",
    "        \n",
    "        # Calculate TD targets and advantages\n",
    "        with torch.no_grad():\n",
    "            _, next_values = self.ac_net(next_states)\n",
    "            next_values = next_values.squeeze()\n",
    "            next_values = next_values * (1 - dones)\n",
    "            td_targets = rewards + self.gamma * next_values\n",
    "            advantages = td_targets - values.detach()\n",
    "            # Optional: Normalize advantages\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Perform multiple epochs of updates (typical for PPO)\n",
    "        for _ in range(self.update_epochs):\n",
    "            # Get current policy and values\n",
    "            policy, current_values = self.ac_net(states)\n",
    "            m = Categorical(policy)\n",
    "            new_log_probs = m.log_prob(actions)\n",
    "            \n",
    "            # Calculate ratios and PPO clipped objective\n",
    "            ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Value loss using TD targets\n",
    "            value_loss = nn.functional.mse_loss(current_values.squeeze(), td_targets)\n",
    "            \n",
    "            # Optional: Add entropy bonus for exploration\n",
    "            entropy_loss = -m.entropy().mean() * 0.01\n",
    "            \n",
    "            # Total loss\n",
    "            loss = policy_loss + 0.5 * value_loss + entropy_loss\n",
    "            \n",
    "            # Update network\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deepstate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
