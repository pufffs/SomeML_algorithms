{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient Theorem Derivation\n",
    "The goal is: \n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\sum_{t=0}^{\\infty} \\gamma^t\\,r(s_t, a_t)\\right]\n",
    "$$\n",
    "\n",
    "We can write:\n",
    "$$\n",
    "J(\\theta) = \\int p_\\theta(\\tau)\\left[\\sum_{t=0}^{\\infty} \\gamma^t\\,r(s_t,a_t)\\right]\\,d\\tau.\n",
    "$$\n",
    "\n",
    "Taking the gradient with respect to $\\theta$:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\int p_\\theta(\\tau)\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right]d\\tau.\n",
    "$$\n",
    "Assuming we can interchange the gradient and the integral:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int \\nabla_\\theta p_\\theta(\\tau)\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right]d\\tau.\n",
    "$$\n",
    "\n",
    "Applying the Log-Likelihood trick gives:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int p_\\theta(\\tau)\\,\\nabla_\\theta \\log p_\\theta(\\tau) \\left[\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right]d\\tau.\n",
    "$$\n",
    "In expectation notation:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\nabla_\\theta \\log p_\\theta(\\tau) \\,\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right].\n",
    "$$\n",
    "\n",
    "\n",
    "Since the trajectory probability factorizes as\n",
    "$$\n",
    "p_\\theta(\\tau) = p(s_0)\\prod_{t=0}^{\\infty} \\left[\\pi_\\theta(a_t \\mid s_t) \\,P(s_{t+1} \\mid s_t,a_t)\\right],\n",
    "$$\n",
    "its logarithm is\n",
    "$$\n",
    "\\log p_\\theta(\\tau) = \\log p(s_0) + \\sum_{t=0}^{\\infty}\\Bigl[\\log \\pi_\\theta(a_t \\mid s_t) + \\log P(s_{t+1} \\mid s_t,a_t)\\Bigr].\n",
    "$$\n",
    "Only the terms $\\log \\pi_\\theta(a_t \\mid s_t)$ depend on $\\theta$. Therefore,\n",
    "$$\n",
    "\\nabla_\\theta \\log p_\\theta(\\tau) = \\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t).\n",
    "$$\n",
    "Substitute back into our gradient expression:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\left(\\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\right)\\left(\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right)\\right].\n",
    "$$\n",
    "\n",
    "For each time step t, decompose the total return as:\n",
    "$$\n",
    "\\sum_{m=0}^{\\infty}\\gamma^m\\,r(s_m,a_m)\n",
    "=\\underbrace{\\sum_{m=0}^{t-1}\\gamma^m\\,r(s_m,a_m)}_{\\text{past (independent of \\(a_t\\))}}\n",
    "+\\underbrace{\\sum_{m=t}^{\\infty}\\gamma^m\\,r(s_m,a_m)}_{\\text{future (dependent on \\(a_t\\))}}.\n",
    "$$\n",
    "Since the past portion is independent of $(a_t)$, its contribution to $(\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t))$ vanishes in expectation. Hence, we have:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\left(\\sum_{m=t}^{\\infty}\\gamma^m\\,r(s_m,a_m)\\right)\\right].\n",
    "$$\n",
    "\n",
    "Since the Q function is:\n",
    "$$\n",
    "Q^\\pi(s_t,a_t) = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty}\\gamma^k\\,r(s_{t+k},a_{t+k}) \\,\\Bigm|\\, s_t,a_t\\right].\n",
    "$$\n",
    "Thus, the partial sum $(\\sum_{m=t}^{\\infty}\\gamma^m\\,r(s_m,a_m)$ is an unbiased sample of $(Q^\\pi(s_t,a_t))$. Therefore,\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\,Q^\\pi(s_t,a_t)\\right].\n",
    "$$\n",
    "\n",
    "\n",
    "Each trajectory $(\\tau)$ consists of a sequence of state–action pairs $((s_0,a_0), (s_1,a_1), \\ldots)$. The sum over time steps is equivalent to taking an expectation with respect to the **discounted occupancy measure** $(\\mu_\\pi(s,a))$, which represents the (normalized) distribution of state–action pairs visited by $(\\pi_\\theta)$. Thus, we can write:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s,a) \\sim \\mu_\\pi}\\left[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)\\,Q^\\pi(s,a)\\right].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Final Policy Gradient Theorem**\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s,a) \\sim \\mu_\\pi}\\left[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)\\,Q^\\pi(s,a)\\right].\n",
    "}\n",
    "$$\n",
    "A common variance-reduced version uses the advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$:\n",
    "$$\n",
    "\\boxed{\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s,a) \\sim \\mu_\\pi}\\left[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)\\,A^\\pi(s,a)\\right].\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE\n",
    "**Objective:**\n",
    "\n",
    "The expected return is given by\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T-1}\\gamma^t\\,r(s_t,a_t)\\right],\n",
    "$$\n",
    "\n",
    "where $ \\tau = (s_0,a_0,\\dots,s_{T-1},a_{T-1}) $ is a complete episode.\n",
    "\n",
    "**Policy Gradient Theorem (using Monte Carlo return):**\n",
    "\n",
    "The gradient is\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) \\, G_t\\right],\n",
    "$$\n",
    "\n",
    "with the return\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{k=t}^{T-1}\\gamma^{k-t}\\,r(s_k,a_k).\n",
    "$$\n",
    "\n",
    "**Update Rule:**\n",
    "\n",
    "For each episode, update\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T-1} G_t \\, \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Critic (A2C)\n",
    "**Architecture:**\n",
    "\n",
    "We have an actor (policy) and a critic (value function). The critic estimates\n",
    "\n",
    "$$\n",
    "V^\\pi(s) \\approx \\mathbb{E}\\left[G_t \\mid s_t = s\\right].\n",
    "$$\n",
    "\n",
    "**Advantage Estimate:**\n",
    "\n",
    "A common choice is the one-step temporal difference (TD) error:\n",
    "\n",
    "$$\n",
    "A_t = r(s_t,a_t) + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t).\n",
    "$$\n",
    "\n",
    "**Loss Functions:**\n",
    "\n",
    "- **Actor Loss:**\n",
    "\n",
    "$$\n",
    "L_{\\text{actor}}(\\theta) = -\\mathbb{E}\\left[\\log \\pi_\\theta(a_t\\mid s_t)\\,A_t\\right].\n",
    "$$\n",
    "\n",
    "- **Critic Loss:**\n",
    "\n",
    "$$\n",
    "L_{\\text{critic}}(\\phi) = \\frac{1}{2}\\mathbb{E}\\left[\\left(r(s_t,a_t) + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t)\\right)^2\\right].\n",
    "$$\n",
    "\n",
    "**Update Rules:**\n",
    "\n",
    "- **Actor Update:**\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha\\,\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) \\, A_t.\n",
    "$$\n",
    "\n",
    "- **Critic Update:**\n",
    "\n",
    "$$\n",
    "\\phi \\leftarrow \\phi - \\beta\\,\\nabla_\\phi L_{\\text{critic}}(\\phi).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Policy Gradient (NPG)\n",
    "**Key Idea:**\n",
    "\n",
    "The standard gradient is preconditioned by the inverse Fisher information matrix $F(\\theta) $ to obtain the natural gradient:\n",
    "\n",
    "$$\n",
    "\\Delta \\theta = F(\\theta)^{-1} \\, \\nabla_\\theta J(\\theta),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "F(\\theta) = \\mathbb{E}_{s \\sim d^\\pi,\\, a \\sim \\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s) \\, \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)^\\top\\right].\n",
    "$$\n",
    "\n",
    "**Update Rule:**\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\, \\Delta \\theta = \\theta + \\alpha \\, F(\\theta)^{-1} \\, \\nabla_\\theta J(\\theta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust Region Policy Optimization (TRPO)\n",
    "The goal of reinforcement learning is to maximize the expected return:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]=\\eta(\\pi_{\\theta})\n",
    "$$\n",
    "Consider the expected return under the new policy $\\pi_{\\theta'}$:\n",
    "\n",
    "$$\n",
    "J(\\theta') = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta'}}[R(\\tau)]\n",
    "$$\n",
    "\n",
    "Using importance sampling with respect to the old policy $\\pi_{\\theta_{\\text{old}}}$, we rewrite:\n",
    "\n",
    "$$\n",
    "J(\\theta') = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\frac{\\pi_{\\theta'}(\\tau)}{\\pi_{\\theta_{\\text{old}}}(\\tau)}R(\\tau)\\right]\n",
    "$$\n",
    "\n",
    "However, directly computing $\\frac{\\pi_{\\theta'}(\\tau)}{\\pi_{\\theta_{\\text{old}}}(\\tau)}$ over entire trajectories is challenging. Therefore, TRPO approximates this by using single-step importance sampling and the advantage function $A^{\\pi_{\\theta_{\\text{old}}}}(s,a)$:\n",
    "\n",
    "$$\n",
    "J(\\theta') \\approx \\eta(\\pi_{\\theta_{\\text{old}}}) + \\sum_s d^{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_a \\pi_{\\theta'}(a|s)A^{\\pi_{\\theta_{\\text{old}}}}(s,a)\n",
    "$$\n",
    "\n",
    "Here, we define the surrogate objective clearly as:\n",
    "\n",
    "$$\n",
    "L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) = \\eta(\\pi_{\\theta_{\\text{old}}}) + \\sum_s d^{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_a \\pi_{\\theta'}(a|s)A^{\\pi_{\\theta_{\\text{old}}}}(s,a)\n",
    "$$\n",
    "\n",
    "Recall the **Conservative Policy Iteration (CPI)** lower bound:\n",
    "$$\n",
    "L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) \\geq \\eta(\\pi_{\\theta_{\\text{old}}}) + \\sum_{s} d^{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_{a}\\pi_{\\theta'}(a|s)A^{\\pi_{\\theta_{\\text{old}}}}(s,a) - \\frac{2\\gamma\\epsilon}{(1-\\gamma)^2} D_{\\text{KL}}^{\\text{max}}(\\pi_{\\theta_{\\text{old}}}\\|\\pi_{\\theta'})\n",
    "$$\n",
    "The CPI bound above is theoretically insightful but practically restrictive due to the term $D_{\\text{KL}}^{\\text{max}}(\\pi||\\pi')$, which requires bounding the divergence at **every state**. \n",
    "\n",
    "In practice, we replace the \"max KL divergence\" with an **average KL divergence** to simplify computation:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}^{\\text{max}}(\\pi_{\\theta_{\\text{old}}}\\|\\pi_{\\theta'}) \\approx \\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)\\|\\pi_{\\theta'}(\\cdot|s))]\n",
    "$$\n",
    "\n",
    "This leads to a more manageable lower bound:\n",
    "\n",
    "$$\n",
    "L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) \\geq \\eta(\\pi_{\\theta_{\\text{old}}}) + \\sum_{s} d^{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_{a}\\pi_{\\theta'}(a|s)A^{\\pi_{\\theta_{\\text{old}}}}(s,a) - \\frac{2\\gamma\\epsilon}{(1-\\gamma)^2}\\cdot\\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)\\|\\pi_{\\theta'}(\\cdot|s))]\n",
    "$$\n",
    "To ensure policy improvement, we aim to maximize the lower bound above. Equivalently, we can pose this as a constrained optimization problem. Specifically, we seek a new policy $\\pi'$ that maximizes (since $\\eta(\\pi_{\\theta_{\\text{old}}})$ is constant):\n",
    "\n",
    "$$\n",
    "L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) = \\sum_{s} d^{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_{a}\\pi_{\\theta'}(a|s)A^{\\pi_{\\theta_{\\text{old}}}}(s,a)=\\mathbb{E}_{s,a \\sim \\pi_{\\theta_{\\text{old}}}} \\left[ \\frac{\\pi_{\\theta'}(a|s)}{\\pi_{\\theta_{old}}(a|s)}A^{\\pi_{\\theta_{\\text{old}}}}(s,a)\\right]\n",
    "$$\n",
    "\n",
    "subject to a constraint on the KL divergence between the old policy and the new policy:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)\\|\\pi_{\\theta'}(\\cdot|s))] \\leq \\delta\n",
    "$$\n",
    "\n",
    "Here, $\\delta$ is a hyperparameter chosen to control how aggressively the policy can change in each update.\n",
    "\n",
    "Thus, the optimization becomes explicitly:\n",
    "\n",
    "$$\n",
    "\\max_{\\pi_{\\theta'}} L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) \\quad\\text{s.t.}\\quad \\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)\\|\\pi_{\\theta'}(\\cdot|s))] \\leq \\delta\n",
    "$$\n",
    "\n",
    "We approximate the surrogate objective around $\\theta_{\\text{old}}$ with a first-order (linear) Taylor expansion:\n",
    "\n",
    "$$\n",
    "L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta'}) \\approx L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta_{\\text{old}}}) + \\nabla_{\\theta} L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta})_{\\theta =\\theta_{old}}^\\top (\\theta' - \\theta_{\\text{old}})\n",
    "$$\n",
    "\n",
    "For the KL divergence constraint, we apply a second-order (quadratic) Taylor expansion around $\\theta_{\\text{old}}$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}}\\left[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)||\\pi_{\\theta'}(\\cdot|s))\\right] \\approx \\frac{1}{2}(\\theta' - \\theta_{\\text{old}})^\\top F(\\theta_{\\text{old}})(\\theta' - \\theta_{\\text{old}})\n",
    "$$\n",
    "\n",
    "where the Fisher Information Matrix (FIM), $F(\\theta_{\\text{old}})$, is defined as:\n",
    "\n",
    "$$\n",
    "F(\\theta_{\\text{old}}) = \\mathbb{E}_{s,a \\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^\\top\\right]\\Big|_{\\theta=\\theta_{\\text{old}}}\n",
    "$$\n",
    "\n",
    "Combining the above approximations yields a simpler constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\max_{\\theta'} \\nabla_{\\theta} L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta})_{\\theta =\\theta_{old}}^\\top(\\theta' - \\theta_{\\text{old}}) \\quad \\text{subject to} \\quad \\frac{1}{2}(\\theta' - \\theta_{\\text{old}})^\\top F(\\theta_{\\text{old}})(\\theta' - \\theta_{\\text{old}}) \\leq \\delta\n",
    "$$\n",
    "\n",
    "Using the method of Lagrange multipliers, we derive the optimal policy update step explicitly as:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\sqrt{\\frac{2\\delta}{g^\\top F^{-1} g}} F^{-1} g\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $g = \\nabla_{\\theta} L_{\\pi_{\\theta_{\\text{old}}}}(\\pi_{\\theta})_{\\theta =\\theta_{old}}$ is the policy gradient evaluated at $\\theta_{old}$ by using the logrithm trick.\n",
    "- $F$ is the Fisher Information Matrix evaluated at $\\theta_{\\text{old}}$.\n",
    "\n",
    "In practice, directly computing the inverse $F^{-1}$ is computationally expensive. Thus, TRPO uses the conjugate gradient method to approximate the product $F^{-1} g$ efficiently without explicitly computing the inverse.\n",
    "\n",
    "---\n",
    "**TRPO Algorithm**\n",
    "\n",
    "**Given:** initial policy parameters $\\theta_0$, KL-divergence constraint parameter $\\delta$\n",
    "\n",
    "**for** iteration $k=0,1,2,\\dots$ **do**:\n",
    "\n",
    "1. **Collect trajectories** by executing policy $\\pi_{\\theta_k}(a|s)$.\n",
    "\n",
    "2. **Compute advantages** $A^{\\pi_{\\theta_k}}(s,a)$ using collected data.\n",
    "\n",
    "3. **Compute policy gradient:**\n",
    "$$\n",
    "g = \\mathbb{E}_{s,a \\sim \\pi_{\\theta_k}}\\left[\\nabla_{\\theta'}\\log\\pi_{\\theta'}(a|s)\\big|_{\\theta'=\\theta_k} A^{\\pi_{\\theta_k}}(s,a)\\right]\n",
    "$$\n",
    "\n",
    "4. **Estimate Fisher Information Matrix (FIM)**:\n",
    "$$\n",
    "F(\\theta_k) = \\mathbb{E}_{s,a \\sim \\pi_{\\theta_k}}\\left[\\nabla_{\\theta_k}\\log\\pi_{\\theta_k}(a|s)\\nabla_{\\theta_k}\\log\\pi_{\\theta_k}(a|s)^\\top\\right]\n",
    "$$\n",
    "\n",
    "5. **Compute policy update direction** by approximately solving:\n",
    "$$\n",
    "F(\\theta_k)x = g\n",
    "$$\n",
    "using the **conjugate gradient method**.\n",
    "\n",
    "6. **Update policy parameters**:\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k + \\sqrt{\\frac{2\\delta}{g^\\top x}}\\,x\n",
    "$$\n",
    "\n",
    "**end for**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
