{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient Theorem Derivation\n",
    "The goal is: \n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\sum_{t=0}^{\\infty} \\gamma^t\\,r(s_t, a_t)\\right]\n",
    "$$\n",
    "\n",
    "We can write:\n",
    "$$\n",
    "J(\\theta) = \\int p_\\theta(\\tau)\\left[\\sum_{t=0}^{\\infty} \\gamma^t\\,r(s_t,a_t)\\right]\\,d\\tau.\n",
    "$$\n",
    "\n",
    "Taking the gradient with respect to $\\theta$:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\int p_\\theta(\\tau)\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right]d\\tau.\n",
    "$$\n",
    "Assuming we can interchange the gradient and the integral:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int \\nabla_\\theta p_\\theta(\\tau)\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right]d\\tau.\n",
    "$$\n",
    "\n",
    "Applying the Log-Likelihood trick gives:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int p_\\theta(\\tau)\\,\\nabla_\\theta \\log p_\\theta(\\tau) \\left[\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right]d\\tau.\n",
    "$$\n",
    "In expectation notation:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\nabla_\\theta \\log p_\\theta(\\tau) \\,\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right].\n",
    "$$\n",
    "\n",
    "\n",
    "Since the trajectory probability factorizes as\n",
    "$$\n",
    "p_\\theta(\\tau) = p(s_0)\\prod_{t=0}^{\\infty} \\left[\\pi_\\theta(a_t \\mid s_t) \\,P(s_{t+1} \\mid s_t,a_t)\\right],\n",
    "$$\n",
    "its logarithm is\n",
    "$$\n",
    "\\log p_\\theta(\\tau) = \\log p(s_0) + \\sum_{t=0}^{\\infty}\\Bigl[\\log \\pi_\\theta(a_t \\mid s_t) + \\log P(s_{t+1} \\mid s_t,a_t)\\Bigr].\n",
    "$$\n",
    "Only the terms $\\log \\pi_\\theta(a_t \\mid s_t)$ depend on $\\theta$. Therefore,\n",
    "$$\n",
    "\\nabla_\\theta \\log p_\\theta(\\tau) = \\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t).\n",
    "$$\n",
    "Substitute back into our gradient expression:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\left(\\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\right)\\left(\\sum_{t=0}^{\\infty}\\gamma^t\\,r(s_t,a_t)\\right)\\right].\n",
    "$$\n",
    "\n",
    "For each time step t, decompose the total return as:\n",
    "$$\n",
    "\\sum_{m=0}^{\\infty}\\gamma^m\\,r(s_m,a_m)\n",
    "=\\underbrace{\\sum_{m=0}^{t-1}\\gamma^m\\,r(s_m,a_m)}_{\\text{past (independent of \\(a_t\\))}}\n",
    "+\\underbrace{\\sum_{m=t}^{\\infty}\\gamma^m\\,r(s_m,a_m)}_{\\text{future (dependent on \\(a_t\\))}}.\n",
    "$$\n",
    "Since the past portion is independent of $(a_t)$, its contribution to $(\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t))$ vanishes in expectation. Hence, we have:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\left(\\sum_{m=t}^{\\infty}\\gamma^m\\,r(s_m,a_m)\\right)\\right].\n",
    "$$\n",
    "\n",
    "Since the Q function is:\n",
    "$$\n",
    "Q^\\pi(s_t,a_t) = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty}\\gamma^k\\,r(s_{t+k},a_{t+k}) \\,\\Bigm|\\, s_t,a_t\\right].\n",
    "$$\n",
    "Thus, the partial sum $(\\sum_{m=t}^{\\infty}\\gamma^m\\,r(s_m,a_m)$ is an unbiased sample of $(Q^\\pi(s_t,a_t))$. Therefore,\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta}\\left[\\sum_{t=0}^{\\infty}\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\,Q^\\pi(s_t,a_t)\\right].\n",
    "$$\n",
    "\n",
    "\n",
    "Each trajectory $(\\tau)$ consists of a sequence of state–action pairs $((s_0,a_0), (s_1,a_1), \\ldots)$. The sum over time steps is equivalent to taking an expectation with respect to the **discounted occupancy measure** $(\\mu_\\pi(s,a))$, which represents the (normalized) distribution of state–action pairs visited by $(\\pi_\\theta)$. Thus, we can write:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s,a) \\sim \\mu_\\pi}\\left[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)\\,Q^\\pi(s,a)\\right].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Final Policy Gradient Theorem**\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s,a) \\sim \\mu_\\pi}\\left[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)\\,Q^\\pi(s,a)\\right].\n",
    "}\n",
    "$$\n",
    "A common variance-reduced version uses the advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$:\n",
    "$$\n",
    "\\boxed{\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s,a) \\sim \\mu_\\pi}\\left[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)\\,A^\\pi(s,a)\\right].\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE\n",
    "**Objective:**\n",
    "\n",
    "The expected return is given by\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T-1}\\gamma^t\\,r(s_t,a_t)\\right],\n",
    "$$\n",
    "\n",
    "where $ \\tau = (s_0,a_0,\\dots,s_{T-1},a_{T-1}) $ is a complete episode.\n",
    "\n",
    "**Policy Gradient Theorem (using Monte Carlo return):**\n",
    "\n",
    "The gradient is\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) \\, G_t\\right],\n",
    "$$\n",
    "\n",
    "with the return\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{k=t}^{T-1}\\gamma^{k-t}\\,r(s_k,a_k).\n",
    "$$\n",
    "\n",
    "**Update Rule:**\n",
    "\n",
    "For each episode, update\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T-1} G_t \\, \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Critic (A2C)\n",
    "**Architecture:**\n",
    "\n",
    "We have an actor (policy) and a critic (value function). The critic estimates\n",
    "\n",
    "$$\n",
    "V^\\pi(s) \\approx \\mathbb{E}\\left[G_t \\mid s_t = s\\right].\n",
    "$$\n",
    "\n",
    "**Advantage Estimate:**\n",
    "\n",
    "A common choice is the one-step temporal difference (TD) error:\n",
    "\n",
    "$$\n",
    "A_t = r(s_t,a_t) + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t).\n",
    "$$\n",
    "\n",
    "**Loss Functions:**\n",
    "\n",
    "- **Actor Loss:**\n",
    "\n",
    "$$\n",
    "L_{\\text{actor}}(\\theta) = -\\mathbb{E}\\left[\\log \\pi_\\theta(a_t\\mid s_t)\\,A_t\\right].\n",
    "$$\n",
    "\n",
    "- **Critic Loss:**\n",
    "\n",
    "$$\n",
    "L_{\\text{critic}}(\\phi) = \\frac{1}{2}\\mathbb{E}\\left[\\left(r(s_t,a_t) + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t)\\right)^2\\right].\n",
    "$$\n",
    "\n",
    "**Update Rules:**\n",
    "\n",
    "- **Actor Update:**\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha\\,\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) \\, A_t.\n",
    "$$\n",
    "\n",
    "- **Critic Update:**\n",
    "\n",
    "$$\n",
    "\\phi \\leftarrow \\phi - \\beta\\,\\nabla_\\phi L_{\\text{critic}}(\\phi).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Policy Gradient (NPG)\n",
    "**Key Idea:**\n",
    "\n",
    "The standard gradient is preconditioned by the inverse Fisher information matrix \\( F(\\theta) \\) to obtain the natural gradient:\n",
    "\n",
    "$$\n",
    "\\Delta \\theta = F(\\theta)^{-1} \\, \\nabla_\\theta J(\\theta),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "F(\\theta) = \\mathbb{E}_{s \\sim d^\\pi,\\, a \\sim \\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s) \\, \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)^\\top\\right].\n",
    "$$\n",
    "\n",
    "**Update Rule:**\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\, \\Delta \\theta = \\theta + \\alpha \\, F(\\theta)^{-1} \\, \\nabla_\\theta J(\\theta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust Region Policy Optimization (TRPO)\n",
    "**Objective:**\n",
    "\n",
    "TRPO seeks to solve\n",
    "\n",
    "$$\n",
    "\\max_\\theta \\; \\hat{\\mathbb{E}}_{(s,a) \\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\frac{\\pi_\\theta(a\\mid s)}{\\pi_{\\theta_{\\text{old}}}(a\\mid s)} \\, A^{\\theta_{\\text{old}}}(s,a)\\right]\n",
    "$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbb{E}}_{s \\sim \\pi_{\\theta_{\\text{old}}}}\\left[D_{KL}\\Bigl(\\pi_{\\theta_{\\text{old}}}(\\cdot\\mid s) \\,\\|\\, \\pi_\\theta(\\cdot\\mid s)\\Bigr)\\right] \\leq \\delta.\n",
    "$$\n",
    "\n",
    "**Solution Method:**\n",
    "\n",
    "TRPO uses a conjugate gradient method and line search to find the update $ \\Delta\\theta $ that approximately solves the constrained problem.\n",
    "\n",
    "**Update:**\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\Delta \\theta.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
